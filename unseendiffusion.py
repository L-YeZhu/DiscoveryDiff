import timeimport globfrom tqdm import tqdmimport osimport numpy as npimport cv2from PIL import Imageimport torchfrom torch import nnimport torchvision.utils as tvufrom sklearn import svmimport pickleimport torch.optim as optimimport randomimport mathimport torch.nn.functional as Fimport scipy.stats as stasfrom scipy.stats import kstest, norm, multivariate_normal, truncnormfrom torch.distributions import normalfrom models.ddpm.diffusion import DDPMfrom models.improved_ddpm.script_util import i_DDPMfrom utils.text_dic import SRC_TRG_TXT_DICfrom utils.diffusion_utils import get_beta_schedule, denoising_stepfrom datasets.data_utils import get_dataset, get_dataloaderfrom configs.paths_config import DATASET_PATHS, MODEL_PATHS, HYBRID_MODEL_PATHS, HYBRID_CONFIGfrom datasets.imagenet_dic import IMAGENET_DICfrom utils.align_utils import run_alignmentfrom utils.distance_utils import euclidean_distance, cosine_similarityfrom torch.nn.functional import normalizedef fit_truncated_gaussian(samples, alpha=0.05):    """    Fits a truncated Gaussian to a set of samples.        Args:    - samples: a PyTorch tensor of shape (N, D), where N is the number of samples and D is the dimensionality.    - alpha: the desired significance level for the confidence interval.        Returns:    - mean: a PyTorch tensor of shape (D,) representing the estimated mean of the truncated Gaussian.    - cov: a PyTorch tensor of shape (D, D) representing the estimated covariance matrix of the truncated Gaussian.    - truncation_range: a float representing the estimated truncation range of the truncated Gaussian.    """        # Calculate the sample mean and covariance    mean = torch.mean(samples, dim=0)    cov = torch.matmul((samples - mean).transpose(0, 1), (samples - mean)) / (samples.shape[0] - 1)    # Calculate the standard deviation of the Mahalanobis distance of the samples    md_std = torch.sqrt(torch.diagonal(torch.matmul(torch.matmul((samples - mean), torch.inverse(cov)), (samples - mean).transpose(0, 1))))    print("check md_std:", md_std)    # Calculate the truncation range using the inverse of the standard normal CDF    truncation_range = torch.max(md_std) * torch.erfinv(1 - alpha)    return mean, cov, truncation_rangedef generate_directions(A, B, N, c):    """    Generates c directional vectors from A in an N-dimensional space such that the directional unit vector AC has an angle of 60 degrees with AB.    Args:    - A (torch.Tensor): A tensor of shape (N,) representing the starting point.    - B (torch.Tensor): A tensor of shape (N,) representing the end point.    - N (int): The dimensional size of the space.    - c (int): The number of generated directions.    Returns:    - directions (torch.Tensor): A tensor of shape (c, N) representing the generated directional vectors.    """    # Calculate vector AB    AB = B - A    # Normalize vector AB    AB_unit = normalize(AB.view(1, -1)).view(-1)    # Calculate rotation matrix for 60 degree rotation around AB_unit    theta = torch.tensor(60.0 * (torch.pi / 180.0))  # convert 60 degrees to radians    cos_theta, sin_theta = torch.cos(theta), torch.sin(theta)    R = torch.eye(N, dtype=torch.float32)    for i in range(N):        for j in range(i+1, N):            R_ij = torch.eye(N, dtype=torch.float32)            R_ij[i,i], R_ij[j,j] = cos_theta, cos_theta            R_ij[i,j], R_ij[j,i] = -sin_theta, sin_theta            R = torch.matmul(R_ij, R)    # Rotate AB_unit to obtain AC_unit    AC_unit = torch.matmul(R, AB_unit.view(-1, 1)).view(-1)    # Sample c directional vectors from A along AC_unit    scaling_factors = torch.linspace(0, 1, c, dtype=torch.float32)    directions = A.view(1, -1) + scaling_factors.view(-1, 1) * AC_unit.view(1, -1)    return directionsdef rej_check(target, tol, input_list):    """    return if every element in input_list is within the acceptance range    """    for i in range(len(input_list)):        ele = input_list[i]        if ele < target - tol or ele > target + tol:            return False    return Truedef truncation(samples, confidence=0.95):    flattened_samples = samples.view(samples.shape[0], -1)    mean = torch.mean(flattened_samples)    cov = torch.cov(flattened_samples)    L = torch.cholesky(cov, upper=True)    L_inv = torch.inverse(L)    norms = torch.norm(flattened_samples - mean, dim = 1)    sorted_norms, indices = torch.sort(norms, descending=True)    alpha = np.sqrt(-2 * np.log(1 - confidence))    threshold = sorted_norms[int(np.floor(len(sorted_norms) * confidence))] + alpha * torch.sqrt(torch.sum(torch.matmul(L_inv, (flattened_samples[indices][:int(np.floor(len(sorted_norms) * confidence))]-mean).T)**2, dim=0))    return threshold.item(), mean, covdef truncated_gaussian(x):    if torch.norm(x - mean) > truncation_range:        return torch.tensor(0.0)    else:        return torch.tensor(fitted_gaussian.pdf(x.numpy()))# def fit_truncated_gaussian(samples, num_std=3.0):#     N = samples.shape[0]#     mean = samples.mean(dim=0)#     cov = torch.zeros((N, 3, 3))#     for i in range(N):#         cov[i] = torch.cov((samples[i] - mean).view(3, -1))#     cov = cov.mean(dim=0)#     # Estimate truncation range#     samples_centered = samples - mean#     prod = torch.matmul(samples_centered, torch.inverse(cov))#     mahal_dist = torch.sum(samples_centered * prod, dim=1)#     max_mahal_dist = torch.max(mahal_dist)#     truncation_range = num_std * torch.sqrt(max_mahal_dist)#     # Fit truncated Gaussian#     truncated_mean = mean#     truncated_cov = cov#     mask = mahal_dist <= max_mahal_dist#     if not torch.all(mask):#         truncated_samples = samples[mask]#         truncated_mean = truncated_samples.mean(dim=0)#         truncated_cov = torch.cov((truncated_samples - truncated_mean).view(-1, 3))#     return truncated_mean, truncated_cov, truncation_range# def fit_truncated_gaussian(samples):#     # Compute the mean and covariance matrix of the samples#     mean = torch.mean(samples, dim=0)#     cov = torch.matmul((samples - mean).transpose(0, 1), (samples - mean)) / (samples.shape[0] - 1)    #     # Compute the truncation range#     cdf_at_bounds = 0.5 * (1 + torch.erf(truncnorm.a / torch.sqrt(cov.diag() * 2)))#     truncation_range = torch.max(torch.norm(mean, dim=0) + truncnorm.a / cdf_at_bounds)    #     # Fit a truncated Gaussian#     lower = torch.zeros_like(mean) - truncation_range#     upper = torch.zeros_like(mean) + truncation_range#     fitted_gaussian = truncnorm(a=lower, b=upper, loc=mean, scale=torch.sqrt(cov.diag()))    #     # Sample from the fitted Gaussian#     return fitted_gaussian.rvs()# def generate_directions(A, B, N, c):#     """#     Generates c directional vectors from A in an N-dimensional space such that the directional unit vector AC has an angle of 60 degrees with AB.#     Args:#     - A (torch.Tensor): A tensor of shape (N,) representing the starting point.#     - B (torch.Tensor): A tensor of shape (N,) representing the end point.#     - N (int): The dimensional size of the space.#     - c (int): The number of generated directions.#     Returns:#     - directions (torch.Tensor): A tensor of shape (c, N) representing the generated directional vectors.#     """#     # Calculate vector AB#     AB = B - A#     # Normalize vector AB#     AB_unit = normalize(AB.view(1, -1)).view(-1)#     # Calculate rotation matrix for 60 degree rotation around AB_unit#     theta = torch.tensor(60.0 * (torch.pi / 180.0))  # convert 60 degrees to radians#     cos_theta, sin_theta = torch.cos(theta), torch.sin(theta)#     R = torch.tensor([[cos_theta + AB_unit[0]**2*(1-cos_theta), AB_unit[0]*AB_unit[1]*(1-cos_theta) - AB_unit[2]*sin_theta, AB_unit[0]*AB_unit[2]*(1-cos_theta) + AB_unit[1]*sin_theta],#                       [AB_unit[1]*AB_unit[0]*(1-cos_theta) + AB_unit[2]*sin_theta, cos_theta + AB_unit[1]**2*(1-cos_theta), AB_unit[1]*AB_unit[2]*(1-cos_theta) - AB_unit[0]*sin_theta],#                       [AB_unit[2]*AB_unit[0]*(1-cos_theta) - AB_unit[1]*sin_theta, AB_unit[2]*AB_unit[1]*(1-cos_theta) + AB_unit[0]*sin_theta, cos_theta + AB_unit[2]**2*(1-cos_theta)]], dtype=torch.float32)#     # Rotate AB_unit to obtain AC_unit#     # R = R.to(self.device)#     AC_unit = torch.matmul(R, AB_unit.view(-1, 1)).view(-1)#     # Sample c directional vectors from A along AC_unit#     scaling_factors = torch.linspace(0, 1, c, dtype=torch.float32)#     directions = A.view(1, -1) + scaling_factors.view(-1, 1) * AC_unit.view(1, -1)#     return directions# def generate_directions(A, B, num_directions):#     # Compute the vector AB#     AB = B - A    #     # Compute the norm of AB#     AB_norm = AB.norm()#     # Compute the unit vector in the direction of AB#     AB_unit = AB / AB_norm#     # Generate a random orthonormal basis for the plane perpendicular to AB#     U, S, V = torch.svd(torch.randn(num_directions, AB_unit.size(0)), some=True)#     W = V.t() @ torch.diag(torch.ones(num_directions)) @ U.t()#     # Generate a set of directional vectors in the plane perpendicular to AB#     directions = W @ torch.tensor([AB_unit.tolist()] * num_directions)#     # Compute the rotation matrix for rotating by 60 degrees about AB#     theta = torch.tensor([60 * (torch.pi / 180)] * num_directions)#     cos_theta = torch.cos(theta)#     sin_theta = torch.sin(theta)#     rotation_matrix = torch.stack([#         torch.stack([cos_theta, -sin_theta, torch.zeros_like(cos_theta)]),#         torch.stack([sin_theta, cos_theta, torch.zeros_like(sin_theta)]),#         torch.stack([torch.zeros_like(theta), torch.zeros_like(theta), torch.ones_like(theta)])#     ], dim=-1)#     # Rotate the directions by the rotation matrix#     directions = directions.unsqueeze(-2) @ rotation_matrix#     directions = directions.squeeze(-2)#     # Normalize the directions#     norms = directions.norm(dim=-1, keepdim=True)#     directions = directions / norms#     return directionsdef find_equilateral_point(A, B, n, lr=0.01, tol=1e-6, max_iter=10000):    with torch.no_grad():        AB = B - A        AB_norm = AB.norm()        n_norm = n.norm()        n = n / n_norm        theta = torch.tensor(math.pi / 3.0)        # Find initial guess for C by moving along the n vector from A        C = A + n * (AB_norm / 2.0 / torch.sin(theta))    C_new = C.clone().detach().requires_grad_(True)    # optimizer = torch.optim.Adam([C_new], lr=lr)    optimizer = torch.optim.SGD([C_new], lr=lr)    for i in range(max_iter):        optimizer.zero_grad()        loss = ((C_new - A).norm() - (C_new - B).norm()) ** 2        loss.backward()        if C_new.grad is not None:            grad_norm = C_new.grad.norm().item()            if grad_norm < tol:                break        optimizer.step()        C_new = C_new.detach().clone().requires_grad_(True)    return C_new.detach()def find_angle(xo_A, xo_B, xo_C):    # angle_list = []    ### Find the mean direction defined by AB and AC    AB = (xo_B - xo_A) / torch.norm(xo_B - xo_A)    AC = (xo_C - xo_A) / torch.norm(xo_C - xo_A)    dot_p = torch.dot(AB.view(-1), AC.view(-1))    AB_magnitude = torch.norm(AB)    AC_magnitude = torch.norm(AC)    angle = torch.acos( dot_p / (AB_magnitude * AC_magnitude))    angle_degrees = angle * 180 / torch.tensor(3.14159)    # print("check dot_p:", torch.sum(dot_p), angle_degrees)    # angle_list.append(angle_degrees)    return angle_degrees    # exit()# angle_list = torch.stack(angle_list)# print("check mean and std of angles:", torch.mean(angle_list), torch.std(angle_list))# def find_equilateral_point(A, B, n, target_angle, lr=0.1, max_iter=1000, tol=1e-6):#     A.requires_grad = True#     B.requires_grad = True#     n.requires_grad = True#     AB = B - A#     AB_norm = torch.norm(AB)#     target_norm = AB_norm / (2 * torch.sin(torch.deg2rad(target_angle / 2)))#     target_direction = AB / AB_norm#     C = A + n * target_norm#     C.requires_grad = True#     optimizer = torch.optim.Adam([C], lr=lr)#     for i in range(max_iter):#         angle = torch.rad2deg(torch.acos(torch.sum((C - A) * (B - A)) / (torch.norm(C - A) * torch.norm(B - A))))#         loss = torch.abs(angle - target_angle)#         if loss < tol:#             break#         optimizer.zero_grad()#         loss.backward()#         optimizer.step()#     return C.detach()# def find_equilateral_point(A, B, n, max_iter=1000, tol=1e-6):#     """#     Find a sample point C along the directional norm vector n, such that A, B, and C form an equilateral triangle.#     Args:#     A (torch.Tensor): a tensor of shape (D,) representing sample point A#     B (torch.Tensor): a tensor of shape (D,) representing sample point B#     n (torch.Tensor): a tensor of shape (D,) representing the directional norm vector with A as one end#     max_iter (int): the maximum number of iterations to perform#     tol (float): the tolerance for convergence#     Returns:#     torch.Tensor: a tensor of shape (D,) representing sample point C#     """#     # Define the objective function#     def objective(C):#         # Calculate the lengths of the sides of the triangle#         AB = torch.norm(B - A)#         AC = torch.norm(C - A)#         BC = torch.norm(C - B)#         # Calculate the difference between the lengths of the sides#         diff = AC - AB#         # Calculate the angle between n and AC#         angle = torch.acos(torch.dot(n, (C - A)) / (torch.norm(n) * AC))#         # Calculate the difference between the target angle and the actual angle#         angle_diff = angle - torch.deg2rad(torch.tensor(60.0))#         # Return the objective value#         return diff ** 2 + angle_diff ** 2#     # Initialize the starting point#     C = A + n#     # Perform gradient descent#     optimizer = torch.optim.SGD([C], lr=0.1)#     for i in range(max_iter):#         optimizer.zero_grad()#         loss = objective(C)#         loss.backward()#         optimizer.step()#         # Check for convergence#         if loss.item() < tol:#             break#     return Cdef compute_radius(x):    x = torch.pow(x, 2)    r = torch.sum(x)    r = torch.sqrt(r)    return rdef slerp(z1, z2, alpha):    theta = torch.acos(torch.sum(z1 * z2) / (torch.norm(z1) * torch.norm(z2)))    return (        torch.sin((1 - alpha) * theta) / torch.sin(theta) * z1        + torch.sin(alpha * theta) / torch.sin(theta) * z2    )def point_hyper(A, B):    # this is the function that returns the param of hyper defined by A and B    # the direction is from A to B, the defining point is B    v = (B - A) / torch.norm(B - A) # the normal vector    bias = -torch.dot(v, B)    # coef = torch.cat([v.cpu(), torch.tensor([bias])], dim=0)    return v.cpu()def lin_interpolation(start, end, num_steps):    step_size = (end - start) / (num_steps - 1)    interpolated_points = []    for i in range(num_steps):        interpolated_point = start + (step_size * i)        interpolated_points.append(interpolated_point)    interpolated_seq = torch.stack(interpolated_points)     # print("check size:", interpolated_seq)    return interpolated_seq# def navigation_iter(start_distance, end_distance, edit_img_number, k, coef_x1, coef_h1, h_s, x_s):#     # start_distance = self.args.start_distance  #     # end_distance = self.args.end_distance#     # edit_img_number = self.args.edit_img_number#     # k is a particular sample #     # print("check editing space:", start_distance, end_distance, edit_img_number, h_s, x_s)#     h_linspace = np.linspace(start_distance, end_distance, edit_img_number)#     h_latent_code = h_s.cpu().view(1,-1).numpy()#     h_linspace = h_linspace - h_latent_code.dot(coef_h1.T)#     h_linspace = h_linspace.reshape(-1, 1).astype(np.float32)#     edit_h_seq = h_latent_code + h_linspace * coef_h1#     z_linspace = np.linspace(start_distance, end_distance, edit_img_number)#     z_latent_code = x_s.cpu().view(1,-1).numpy()#     z_linspace = z_linspace - z_latent_code.dot(coef_x1.T)#     z_linspace = z_linspace.reshape(-1, 1).astype(np.float32)#     edit_z_seq = z_latent_code + z_linspace * coef_x1 #     x_h = torch.from_numpy(edit_h_seq[k]).view(-1, 512, 8, 8)#     x_s = torch.from_numpy(edit_z_seq[k]).view(-1, 3, 256, 256)#     return edit_h_seq, edit_z_seq, x_h, x_sdef navigation_iter(start_distance, end_distance, edit_img_number, coef_x1, x_s):    # start_distance = self.args.start_distance      # end_distance = self.args.end_distance    # edit_img_number = self.args.edit_img_number    # k is a particular sample     # print("check editing space:", start_distance, end_distance, edit_img_number, h_s, x_s)    # h_linspace = np.linspace(start_distance, end_distance, edit_img_number)    # h_latent_code = h_s.cpu().view(1,-1).numpy()    # h_linspace = h_linspace - h_latent_code.dot(coef_h1.T)    # h_linspace = h_linspace.reshape(-1, 1).astype(np.float32)    # edit_h_seq = h_latent_code + h_linspace * coef_h1    z_linspace = np.linspace(start_distance, end_distance, edit_img_number)    z_latent_code = x_s.cpu().view(1,-1).numpy()    z_linspace = z_linspace - z_latent_code.dot(coef_x1.T)    z_linspace = z_linspace.reshape(-1, 1).astype(np.float32)    edit_z_seq = z_latent_code + z_linspace * coef_x1     # x_h = torch.from_numpy(edit_h_seq[k]).view(-1, 512, 8, 8)    x_s = torch.from_numpy(edit_z_seq[-1]).view(-1, 3, 256, 256)    return edit_z_seq, x_sdef point_to_subspace(point, normals):    """    Project a point to the subspace defined by a set of hyperplanes    normals a tensor of shape (N,D)    """    N, D = normals.shape    coef = torch.matmul(normals, point)    coef /= torch.norm(normals, dim=1)    proj = coef.view(N,1) * normals    subspace_point = torch.sum(proj, dim=0)    print("check subspace_point:", subspace_point.size())    return subspace_pointdef sample_spherical_shell(radius, ndim, nsamples):    x = np.random.normal(size=(nsamples, ndim))    norm = np.linalg.norm(x, axis=1)    x *= radius / norm[:, None]    return xdef find_point_c(a, b):    # this is a recursive method    ab = b - a    c = torch.randn_like(ab)    c -= torch.dot(c, ab) / torch.dot(ab, ab) * ab    c /= torch.norm(c)    ac = c - a    bc = c - b    abac_angle = torch.acos(torch.dot(ab, ac) / (torch.norm(ab) * torch.norm(ac)))    abbc_angle = torch.acos(torch.dot(ab, bc) / (torch.norm(ab) * torch.norm(bc)))    acbc_angle = torch.acos(torch.dot(ac, bc) / (torch.norm(ac) * torch.norm(bc)))    if torch.allclose(abac_angle, torch.tensor([math.pi / 3.0])):        if torch.allclose(abbc_angle, torch.tensor([math.pi / 3.0])):            if torch.allclose(acbc_angle, torch.tensor([math.pi / 3.0])):                return c    return find_point_c(a, b)def find_point_c_iterative(A, B):    # this is iterative    def angle_between(v1, v2):        cos_theta = torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))        return math.acos(cos_theta)    target_angle = math.radians(60)    tolerance = 0.5    def angle_difference(angle):        return abs(angle - target_angle)    step_size = torch.norm(B - A) / 100    C = A.clone()    AB = B - A    AB_norm = torch.norm(AB)    AB_unit = AB / AB_norm    while True:        rand_vec = torch.randn_like(A)        rand_vec_prep = rand_vec - AB_unit * torch.dot(rand_vec, AB_unit)        angle = angle_between(rand_vec_prep, AB_unit)        if angle_difference(angle) < tolerance:            break        C += rand_vec_prep * (step_size / torch.sin(angle))        AC = C - A        BC = C - B        if torch.dot(AC, AB) < 0 or torch.dot(BC, -AB) < 0:            break    return Cdef find_C(A, B, C = None, tolerance=0.1, max_iter=10000000):    AB = B - A    AB_norm = AB.norm()    # if C == None:    #     C = torch.randn_like(A)    # else:        # C = A + B     C = (A + B) / 2    for i in range(max_iter):        AC = C - A        BC = C - B        AC_norm = AC.norm()        BC_norm = BC.norm()        if AC_norm == 0 or BC_norm == 0:            print("zero norm")            return None        cos_angle = torch.dot(AC, BC) / (AC_norm * BC_norm)        angle = torch.acos(cos_angle)        if torch.abs(angle - (torch.tensor(2 * torch.pi / 3))) <= tolerance:            return C, i        grad1 = (BC_norm * torch.cos(torch.tensor(2 * torch.pi / 3)) * AC / AB_norm -                 torch.sin(torch.tensor(2 * torch.pi / 3)) * BC / BC_norm)        grad2 = (AC_norm * torch.cos(torch.tensor(2 * torch.pi / 3)) * BC / AB_norm -                 torch.sin(torch.tensor(2 * torch.pi / 3)) * AC / AC_norm)        grad = grad1 + grad2        C -= grad    print("Reach maximum iter!")    return Nonedef find_C_withD(A, B, D, target_angle, AD_dir, tolerance=1e-2, max_iter=10000):    # Compute the target distance AC based on the target angle and AB length    AB_norm = torch.norm(B - A)    target_AC_norm = AB_norm / torch.sqrt(torch.tensor(3))    # Define the starting position for C as the midpoint of AD    AC_dir = torch.cross(AD_dir, B - A)    AC_dir /= torch.norm(AC_dir)    C = A + AC_dir * target_AC_norm / 2    # Define the step size for updating C    step_size = target_AC_norm / 2    # Run the optimization loop to find C    for i in range(max_iter):        # Compute the angle between vectors AB and AC        AB = B - A        AC = C - A        cos_angle = torch.dot(AB, AC) / (torch.norm(AB) * torch.norm(AC))        angle = torch.acos(cos_angle)        # If the angle is within tolerance of the target angle, return C        if torch.abs(angle - target_angle) < tolerance:            return C, i        # Compute the gradient of the angle with respect to C        grad1 = (AB / (torch.norm(AB)**2)) - ((torch.dot(AB, AC) / (torch.norm(AB)**2)) * (AC / (torch.norm(AC)**2)))        grad2 = (torch.cross(grad1, AB))        grad = grad2 / torch.norm(grad2)        # Update the position of C using gradient descent        C += step_size * grad    # If max_iter is reached without finding a solution, return None    return Noneclass UnseenDiffusion(object):    def __init__(self, args, config, device=None):        self.args = args        self.config = config        if device is None:            device = torch.device(                "cuda") if torch.cuda.is_available() else torch.device("cpu")        self.device = device        self.model_var_type = config.model.var_type        betas = get_beta_schedule(            beta_start=config.diffusion.beta_start,            beta_end=config.diffusion.beta_end,            num_diffusion_timesteps=config.diffusion.num_diffusion_timesteps        )        self.betas = torch.from_numpy(betas).float().to(self.device)        self.num_timesteps = betas.shape[0]        self.center = torch.zeros((3, 256, 256)).float().to(self.device)        alphas = 1.0 - betas        self.alphas = torch.from_numpy(alphas).float().to(self.device)        alphas_cumprod = np.cumprod(alphas, axis=0)        self.alphas_cumprod = torch.from_numpy(alphas_cumprod).float().to(self.device)        variances = 1.0 - alphas_cumprod        self.variances = torch.from_numpy(variances).float().to(self.device)        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])        posterior_variance = betas * \                             (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)        if self.model_var_type == "fixedlarge":            self.logvar = np.log(np.append(posterior_variance[1], betas[1:]))        elif self.model_var_type == 'fixedsmall':            self.logvar = np.log(np.maximum(posterior_variance, 1e-20))        if self.args.edit_attr is None:            self.src_txts = self.args.src_txts            self.trg_txts = self.args.trg_txts        else:            self.src_txts = SRC_TRG_TXT_DIC[self.args.edit_attr][0]            self.trg_txts = SRC_TRG_TXT_DIC[self.args.edit_attr][1]    def unseen_reconstruct(self):        print(self.args.exp)        # ----------- Model -----------#        if self.config.data.dataset == "LSUN":            if self.config.data.category == "bedroom":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt"            elif self.config.data.category == "church_outdoor":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt"        elif self.config.data.dataset == "CelebA_HQ":            url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt"        elif self.config.data.dataset == "AFHQ":            pass        else:            raise ValueError        if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:            model = DDPM(self.config)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)            learn_sigma = False            print("Original diffusion Model loaded.")        elif self.config.data.dataset in ["FFHQ", "AFHQ"]:            model = i_DDPM(self.config.data.dataset)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])            learn_sigma = True            print("Improved diffusion Model loaded.")        else:            print('Not implemented dataset')            raise ValueError        model.load_state_dict(init_ckpt)        model.to(self.device)        model = torch.nn.DataParallel(model)        model.eval()        # ----------- Precompute Latents -----------#        seq_inv = np.linspace(0, 1, 999) * 999        seq_inv = [int(s) for s in list(seq_inv)]        seq_inv_next = [-1] + list(seq_inv[:-1])        ### get an arbitrary img        n = 1        im = "./celeb_origin.jpg"        im = ood_imgs[100]        img = Image.open(im).convert("RGB")        img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        img = np.array(img)/255        img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        img = img.to(self.config.device)        img_name = 'orig.png'        x0 = (img - 0.5) * 2.        tvu.save_image((x0 + 1) * 0.5, os.path.join("results",img_name))        with torch.no_grad():            #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#            if self.args.deterministic_inv:                seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0                seq_inv = [int(s) for s in list(seq_inv)]                seq_inv_next = [-1] + list(seq_inv[:-1])                x = x0.clone()                with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:                    for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):                        t = (torch.ones(n) * i).to(self.device)                        t_prev = (torch.ones(n) * j).to(self.device)                        x, _ = denoising_step(x, t=t, t_next=t_prev, models=model,                                           logvars=self.logvar,                                           sampling_type='ddim',                                           b=self.betas,                                           eta=0,                                           learn_sigma=learn_sigma,                                           ratio=0,                                           )                        progress_bar.update(1)                    x_lat = x.clone()              time_in_start = time.time()        with torch.no_grad():            with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:                for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):                    t = (torch.ones(n) * i).to(self.device)                    t_next = (torch.ones(n) * j).to(self.device)                    x_lat, _ = denoising_step(x_lat, t=t, t_next=t_next, models=model,                                       logvars=self.logvar,                                       # sampling_type=self.args.sample_type,                                       sampling_type='ddim',                                       b=self.betas,                                       eta=0.1,                                       learn_sigma=learn_sigma,                                       )                    progress_bar.update(1)                x0 = x_lat.clone()                save_edit = "unseen_recons_eta_01.png"                tvu.save_image((x0 + 1) * 0.5, os.path.join("results",save_edit))                time_in_end = time.time()                print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")        return    def unconditional(self):        print(self.args.exp)        # ----------- Model -----------#        if self.config.data.dataset == "LSUN":            if self.config.data.category == "bedroom":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt"            elif self.config.data.category == "church_outdoor":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt"        elif self.config.data.dataset == "CelebA_HQ":            url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt"        elif self.config.data.dataset == "AFHQ":            pass        else:            raise ValueError        if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:            model = DDPM(self.config)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)            learn_sigma = False            print("Original diffusion Model loaded.")        elif self.config.data.dataset in ["FFHQ", "AFHQ"]:            model = i_DDPM(self.config.data.dataset)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])            learn_sigma = True            print("Improved diffusion Model loaded.")        else:            print('Not implemented dataset')            raise ValueError        model.load_state_dict(init_ckpt)        model.to(self.device)        model = torch.nn.DataParallel(model)        model.eval()        # ----------- Precompute Latents -----------#        seq_inv = np.linspace(0, 1, 999) * 999        seq_inv = [int(s) for s in list(seq_inv)]        seq_inv_next = [-1] + list(seq_inv[:-1])                      x_lat = torch.randn(10, 3, 256, 256, device=self.device)        n = 10        print("get the sampled latent encodings x_T!")        time_in_start = time.time()        with torch.no_grad():            with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:                for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):                    t = (torch.ones(n) * i).to(self.device)                    t_next = (torch.ones(n) * j).to(self.device)                    x_lat, h_lat = denoising_step(x_lat, t=t, t_next=t_next, models=model,                                       logvars=self.logvar,                                       # sampling_type=self.args.sample_type,                                       sampling_type='ddim',                                       b=self.betas,                                       eta=0.0,                                       learn_sigma=learn_sigma,                                       )                    progress_bar.update(1)                x0 = x_lat.clone()                save_edit = "sampled.png"                tvu.save_image((x0 + 1) * 0.5, os.path.join("results",save_edit))                time_in_end = time.time()                print(f"Sampling for 1 image takes {time_in_end - time_in_start:.4f}s")        return    def estimation(self):        print(self.args.exp)        # ----------- Model -----------#        if self.config.data.dataset == "LSUN":            if self.config.data.category == "bedroom":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt"            elif self.config.data.category == "church_outdoor":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt"        elif self.config.data.dataset == "CelebA_HQ":            url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt"        elif self.config.data.dataset == "AFHQ":            pass        else:            raise ValueError        if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:            model = DDPM(self.config)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)            learn_sigma = False            print("Original diffusion Model loaded.")        elif self.config.data.dataset in ["FFHQ", "AFHQ"]:            model = i_DDPM(self.config.data.dataset)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])            learn_sigma = True            print("Improved diffusion Model loaded.")        else:            print('Not implemented dataset')            raise ValueError        model.load_state_dict(init_ckpt)        model.to(self.device)        model = torch.nn.DataParallel(model)        model.eval()        # ---- check betas --- #        # print("check variances:", self.variances, len(self.variances))        means = torch.sqrt(self.alphas_cumprod)         # print("check means:", self.alphas_cumprod, len(self.alphas_cumprod))        ## define the param. of Gaussian        mean_t = means[500].to(self.device)        var_t = self.variances[500]        print("check mean and variance at space t:", mean_t, var_t)        # exit()        # ----------- Precompute Latents -----------#        # seq_inv = np.linspace(0, 1, 999) * 999        seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        seq_inv = [int(s) for s in list(seq_inv)]        seq_inv_next = [-1] + list(seq_inv[:-1])        print("check the seq_inv:", seq_inv, len(seq_inv))        ## are they separable? Two Gaussians are separable with central distance larger than d^(1/4)        ### Q1: How many samples do we need to estimate? ID and OOD        ### Let's try with 100 samples        #### ID cases - get ID images        id_imgs = glob.glob("/n/fs/visualai-scr/Data/AFHQ-Dog/afhq/train/dog/*")        random.shuffle(id_imgs)        id_x0_1 = torch.zeros(2000,3,256,256).float().to(self.device)        # id_x0_2 = torch.zeros(200,3,256,256).float()        print("check id imgs:", len(id_imgs))        ##### Two directions estimation, theoretical (using x0) and empirical (using xT)        for c, im in enumerate(id_imgs[0:2000]):            img = Image.open(im).convert("RGB")            img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)            img = np.array(img)/255            img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(1, 1, 1, 1)            img = img.to(self.config.device)            x0 = (img - 0.5) * 2.            id_x0_1[c,:,:,:] = x0                inverted_id_c = torch.mean(id_x0_1, dim=0) * mean_t        distance_id = torch.sqrt(torch.sum(inverted_id_c) ** 2)        print("check inverted_id_c:", inverted_id_c.size())        print("check sampled and inverted_theo distance for ID:", distance_id)        #### OOD cases - get OOD images - use human faces as examples        # ood_imgs = glob.glob("/n/fs/visualai-scr/Data/CelebA-HQ/raw_images/train/images/*")        ood_imgs = glob.glob("/n/fs/visualai-scr/Data/LSUN2_imgs/bedroom/train_imgs/*")        random.shuffle(ood_imgs)        ood_x0_1 = torch.zeros(2000,3,256,256).float().to(self.device)        print("check ood imgs:", len(ood_imgs)) #29000        ### check geometric properties in raw image space        for c, im in enumerate(ood_imgs[0:2000]):            img = Image.open(im).convert("RGB")            img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)            img = np.array(img)/255            img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(1, 1, 1, 1)            img = img.to(self.config.device)            x0 = (img - 0.5) * 2.            ood_x0_1[c,:,:,:] = x0                inverted_ood_c = torch.mean(ood_x0_1, dim=0) #* mean_t        distance_ood = torch.sqrt(torch.sum(inverted_ood_c) ** 2)        print("check inverted_ood_c:", inverted_ood_c.size())        print("check sampled ID and inverted_theo distance for OOD:", distance_ood)        ### distance between ID and OOD from theory        distance_center = torch.sqrt(torch.sum(inverted_id_c - inverted_ood_c) ** 2)        print("final center diatance check:", distance_center)        # exit()        #### test w/ unseen sampling        # x_lat = torch.randn(10, 3, 256, 256, device=self.device)        # x_lat = x_lat + inverted_ood_c        # n = 10        # G_ood = normal.Normal()        x_lat = torch.normal(mean=inverted_ood_c, std=var_t)        x_lat = x_lat.unsqueeze(0).to(self.device)        n=1        print("check x_lat:", x_lat.size())        print("get the sampled latent encodings x_T!")        # exit()        # time_in_start = time.time()        with torch.no_grad():            with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:                for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):                    t = (torch.ones(n) * i).to(self.device)                    t_next = (torch.ones(n) * j).to(self.device)                    x_lat, h_lat = denoising_step(x_lat, t=t, t_next=t_next, models=model,                                       logvars=self.logvar,                                       # sampling_type=self.args.sample_type,                                       sampling_type='ddim',                                       b=self.betas,                                       eta=0.0,                                       learn_sigma=learn_sigma,                                       )                    progress_bar.update(1)                x0 = x_lat.clone()                print("check x0 size:", x0.size())                # exit()                for k in range(n):                    save_edit = str(k)+"_test_unseen_bedroom_sampled.png"                    tvu.save_image((x0[k,:,:,:] + 1) * 0.5, os.path.join("results",save_edit))                    # time_in_end = time.time()                    # print(f"Sampling for 1 image takes {time_in_end - time_in_start:.4f}s")        return    # def unseen_sampling(self):    #     print(self.args.exp)    #     # ----------- Model -----------#    #     if self.config.data.dataset == "LSUN":    #         if self.config.data.category == "bedroom":    #             url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt"    #         elif self.config.data.category == "church_outdoor":    #             url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt"    #     elif self.config.data.dataset == "CelebA_HQ":    #         url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt"    #     elif self.config.data.dataset == "AFHQ":    #         pass    #     else:    #         raise ValueError    #     if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:    #         model = DDPM(self.config)    #         if self.args.model_path:    #             init_ckpt = torch.load(self.args.model_path)    #         else:    #             init_ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)    #         learn_sigma = False    #         print("Original diffusion Model loaded.")    #     elif self.config.data.dataset in ["FFHQ", "AFHQ"]:    #         model = i_DDPM(self.config.data.dataset)    #         if self.args.model_path:    #             init_ckpt = torch.load(self.args.model_path)    #         else:    #             init_ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])    #         learn_sigma = True    #         print("Improved diffusion Model loaded.")    #     else:    #         print('Not implemented dataset')    #         raise ValueError    #     model.load_state_dict(init_ckpt)    #     model.to(self.device)    #     model = torch.nn.DataParallel(model)    #     model.eval()    #     # ----------- Precompute Latents -----------#    #     seq_inv = np.linspace(0, 1, 999) * 999    #     seq_inv = [int(s) for s in list(seq_inv)]    #     seq_inv_next = [-1] + list(seq_inv[:-1])                  #     x_lat = torch.randn(1, 3, 256, 256, device=self.device)    #     x_lat = x_lat +     #     n = 1    #     print("get the sampled latent encodings x_T!")    #     time_in_start = time.time()    #     with torch.no_grad():    #         with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:    #             for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):    #                 t = (torch.ones(n) * i).to(self.device)    #                 t_next = (torch.ones(n) * j).to(self.device)    #                 x_lat, h_lat = denoising_step(x_lat, t=t, t_next=t_next, models=model,    #                                    logvars=self.logvar,    #                                    # sampling_type=self.args.sample_type,    #                                    sampling_type='ddim',    #                                    b=self.betas,    #                                    eta=0.0,    #                                    learn_sigma=learn_sigma,    #                                    )    #                 progress_bar.update(1)    #             x0 = x_lat.clone()    #             save_edit = "test_unseen_sampled.png"    #             tvu.save_image((x0 + 1) * 0.5, os.path.join("results",save_edit))    #             time_in_end = time.time()    #             print(f"Sampling for 1 image takes {time_in_end - time_in_start:.4f}s")    #     return    def radius(self):        print(self.args.exp)        # ----------- Model -----------#        if self.config.data.dataset == "LSUN":            if self.config.data.category == "bedroom":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt"            elif self.config.data.category == "church_outdoor":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt"        elif self.config.data.dataset == "CelebA_HQ":            url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt"        elif self.config.data.dataset == "AFHQ":            pass        else:            raise ValueError        if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:            model = DDPM(self.config)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)            learn_sigma = False            print("Original diffusion Model loaded.")        elif self.config.data.dataset in ["FFHQ", "AFHQ"]:            model = i_DDPM(self.config.data.dataset)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])            learn_sigma = True            print("Improved diffusion Model loaded.")        else:            print('Not implemented dataset')            raise ValueError        model.load_state_dict(init_ckpt)        model.to(self.device)        model = torch.nn.DataParallel(model)        model.eval()        # ---------- Prepare the seq --------- #        # seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        seq_inv = np.linspace(0, 1, 999) * 999        seq_inv = [int(s) for s in list(seq_inv)]        seq_inv_next = [-1] + list(seq_inv[:-1])        # print("check seq_inv:", seq_inv)        # print("length:", len(seq_inv))        n = 1        with torch.no_grad():            er = 0            x_rand = torch.randn(100, 3, 256, 256, device=self.device)            for idx in range(100):                x = x_rand[idx, :, :, :].unsqueeze(0)                with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:                    for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):                        t = (torch.ones(n) * i).to(self.device)                        t_next = (torch.ones(n) * j).to(self.device)                        if t == 500:                            break                        x, _ = denoising_step(x, t=t, t_next=t_next, models=model,                                           logvars=self.logvar,                                           # sampling_type=self.args.sample_type,                                           sampling_type='ddim',                                           b=self.betas,                                           eta=0.0,                                           learn_sigma=learn_sigma,                                           )                        progress_bar.update(1)                    r_x = compute_radius(x)                er += r_x                # save_path = "radius_"+str(idx)+".png"                # tvu.save_image((x + 1) * 0.5, save_path)        print("Check radius at step :", er/100)        # # ----------- Precompute Latents -----------#        # print("Prepare identity latent")        # seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        # seq_inv = [int(s) for s in list(seq_inv)]        # seq_inv_next = [-1] + list(seq_inv[:-1])        # # print("check the seq_inv:", seq_inv)        # train_dataset, test_dataset = get_dataset(self.config.data.dataset, DATASET_PATHS, self.config)        # loader_dic = get_dataloader(train_dataset, test_dataset, bs_train=self.args.bs_train,        #                             num_workers=self.config.data.num_workers)         # # print("")        # n = self.args.bs_train        # # er = 0        # for step, (img,_) in enumerate(loader_dic['train']):        #     print("count samples:", step+1)        #     x0 = img.to(self.config.device)        #     save_image_original = './unconditional/'+str(step)+'.png'        #     tvu.save_image((x0 + 1) * 0.5, save_image_original)        #     x = x0.clone()        #     model.eval()          #     with torch.no_grad():        #         with tqdm(total=len(seq_inv), desc=f"Inversion process {step}") as progress_bar:        #             for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_prev = (torch.ones(n) * j).to(self.device)        #                 x, mid_h_g = denoising_step(x, t=t, t_next=t_prev, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type='ddim',        #                                    b=self.betas,        #                                    eta=0,        #                                    learn_sigma=learn_sigma)        #                 progress_bar.update(1)        #         # x = torch.rand(1, 3, 256, 256, device=self.device)        #         with tqdm(total=len(seq_inv), desc=f"Generative process {step}") as progress_bar:        #             for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 x, _ = denoising_step(x, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    # sampling_type=self.args.sample_type,        #                                    sampling_type='ddpm',        #                                    b=self.betas,        #                                    eta=1,        #                                    learn_sigma=learn_sigma,        #                                    )        #                 progress_bar.update(1)           #         save_path = "./unconditional/uncondition_guided_"+str(step)+".png"        #         tvu.save_image((x + 1) * 0.5, save_path)        #         # exit()                                   return    def spatial(self):        print(self.args.exp)        n = self.args.bs_test        print("check n:", n)        # ----------- Model -----------#        if self.config.data.dataset == "LSUN":            if self.config.data.category == "bedroom":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt"            elif self.config.data.category == "church_outdoor":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt"        elif self.config.data.dataset == "CelebA_HQ":            url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt"        elif self.config.data.dataset == "AFHQ":            pass        else:            raise ValueError        if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:            model = DDPM(self.config)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)            learn_sigma = False            print("Original diffusion Model loaded.")        elif self.config.data.dataset in ["FFHQ", "AFHQ"]:            model = i_DDPM(self.config.data.dataset)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])            learn_sigma = True            print("Improved diffusion Model loaded.")        else:            print('Not implemented dataset')            raise ValueError        model.load_state_dict(init_ckpt)        model.to(self.device)        model = torch.nn.DataParallel(model)        model.eval()        ######## spatial geometric check ###########        # imgs = glob("/n/fs/yz-diff/BoundaryDiffusion/syn_imgs/train/panda/pos/*")        ood_imgs = glob.glob("/n/fs/visualai-scr/Data/CelebA-HQ/raw_images/train/images/*")        random.shuffle(ood_imgs)        origin = torch.zeros((3,256,256)).type(torch.FloatTensor)        ood_x = torch.empty(1000,3,256,256)        for c, im in enumerate(ood_imgs[0:1000]):            img = Image.open(im).convert("RGB")            img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)            img = np.array(img)/255            img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)            img = img.to(self.config.device)            x0 = (img - 0.5) * 2.            ood_x[c, :, :, :] = x0        x_dis_list = []        x_angle_list = []        count = 0        for i in range(1000):            p = random.randint(0,999)            q = random.randint(0,999)            a = random.randint(0,999)            if p != q and p != a and q != a:            # if p != q:                print("random ood sample idx:", count, p, q)                count += 1                ood_p_x = ood_x[p,:,:,:]                ood_q_x = ood_x[q,:,:,:]                ood_a_x = ood_x[a,:,:,:]                x_dis = euclidean_distance(ood_p_x.reshape(-1), ood_q_x.reshape(-1))                x_angle = find_angle(origin, ood_p_x, ood_q_x)                x_dis_list.append(x_dis)                x_angle_list.append(x_angle)                print("check:", x_dis, x_angle)                # exit()        #         # exit()        mean_dis = sum(x_dis_list)/len(x_dis_list)        mean_ang = sum(x_angle_list)/len(x_angle_list)        print("check distance and angles:", mean_dis, mean_ang)            # with torch.no_grad():            #     #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#            #     if self.args.deterministic_inv:            #         # x_lat_path = os.path.join(self.args.image_folder, f'x_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}.pth')            #         # h_lat_path = os.path.join(self.args.image_folder, f'h_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}.pth')            #         # if not os.path.exists(x_lat_path):            #         seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0            #         seq_inv = [int(s) for s in list(seq_inv)]            #         seq_inv_next = [-1] + list(seq_inv[:-1])            #         x = x0.clone()            #         with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:            #             for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):            #                 t = (torch.ones(n) * i).to(self.device)            #                 t_prev = (torch.ones(n) * j).to(self.device)            #                 x, _ = denoising_step(x, t=t, t_next=t_prev, models=model,            #                                    logvars=self.logvar,            #                                    sampling_type='ddim',            #                                    b=self.betas,            #                                    eta=0,            #                                    learn_sigma=learn_sigma,            #                                    ratio=0,            #                                    )            #                 progress_bar.update(1)            #             x_lat = x.clone()                        # torch.save(x_lat, x_lat_path)                        # torch.save(h_lat, h_lat_path)                    # print("Finish inversion for the given image to step:", self.args.t_0, x_lat.size(), h_lat.size(), c)            # ood_x[c, :, :, :] = x_0            # od_radius += euclidean_distance(x_lat.reshape(-1), od_c.reshape(-1))            # orgin_radius += compute_radius(x_lat)            # ood_h[c, :, :, :] = h_lat        # with torch.no_grad():            # # get an ID sample with stochastic process            # seq_inv = np.linspace(0, 1, 999) * 999            # seq_inv = [int(s) for s in list(seq_inv)]            # seq_inv_next = [-1] + list(seq_inv[:-1])            # x_s = torch.randn(1, 3, 256, 256, device=self.device)            # with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:            #     for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):            #         t = (torch.ones(n) * i).to(self.device)            #         t_next = (torch.ones(n) * j).to(self.device)            #         # print("check step at generation:", t, t_next)            #         if t == self.args.t_0:            #             break            #         x_s, _ = denoising_step(x_s, t=t, t_next=t_next, models=model,            #                            logvars=self.logvar,            #                            sampling_type=self.args.sample_type,            #                            # sampling_type='ddim',            #                            b=self.betas,            #                            eta=1.0,            #                            learn_sigma=learn_sigma,            #                            )            #         progress_bar.update(1)            #     print("check x_s and h_s for ID samples:", x_s.size(), h_s.size())            #     print("check distance between ID and OOD at eplison level:", euclidean_distance(x_lat.reshape(-1), x_s.reshape(-1)))            #     print("check distance between ID and OOD at h level:", euclidean_distance(h_lat.reshape(-1), h_s.reshape(-1)))                # exit()        # print("Check radius, euc_dis, cos_sim at step :", er/100, euc_dis/100, cos_sim/100)    def unseen_traj(self):        print(self.args.exp)        # ----------- Model -----------#        if self.config.data.dataset == "LSUN":            if self.config.data.category == "bedroom":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt"            elif self.config.data.category == "church_outdoor":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt"        elif self.config.data.dataset == "CelebA_HQ":            url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt"        elif self.config.data.dataset == "AFHQ":            pass        else:            raise ValueError        if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:            model = DDPM(self.config)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)            learn_sigma = False            print("Original diffusion Model loaded.")        elif self.config.data.dataset in ["FFHQ", "AFHQ"]:            model = i_DDPM(self.config.data.dataset)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])            learn_sigma = True            print("Improved diffusion Model loaded.")        else:            print('Not implemented dataset')            raise ValueError        model.load_state_dict(init_ckpt)        model.to(self.device)        model = torch.nn.DataParallel(model)        model.eval()            n = self.args.bs_test        # prepare an ID sampled latent encoding, get x_s and h_s         # n = 1        ####### Many to One Mapping        ## get one single OOD image        # img = Image.open("/n/fs/yz-diff/BoundaryDiffusion/syn_imgs/train/human/pos/190846.jpg").convert("RGB")        # img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        # img = np.array(img)/255        # img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        # img = img.to(self.config.device)        # x0 = (img - 0.5) * 2.        # with torch.no_grad():        #     #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#        #     if self.args.deterministic_inv:        #         seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        #         seq_inv = [int(s) for s in list(seq_inv)]        #         seq_inv_next = [-1] + list(seq_inv[:-1])        #         x = x0.clone()        #         with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:        #             for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_prev = (torch.ones(n) * j).to(self.device)        #                 x, mid_h_g = denoising_step(x, t=t, t_next=t_prev, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type='ddim',        #                                    b=self.betas,        #                                    eta=0,        #                                    learn_sigma=learn_sigma,        #                                    ratio=0,        #                                    )        #                 progress_bar.update(1)        #             x_lat = x.clone()        #             h_lat = mid_h_g.clone()                #     # get many ID samples with stochastic process        #     seq_inv = np.linspace(0, 1, 999) * 999        #     seq_inv = [int(s) for s in list(seq_inv)]        #     seq_inv_next = [-1] + list(seq_inv[:-1])        #     x_s_batch = torch.randn(50, 3, 256, 256, device=self.device)        #     for s in range(50):        #         x_s = x_s_batch[s,:,:,:].unsqueeze(0)        #         with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:        #             for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 # print("check step at generation:", t, t_next)        #                 if t == self.args.t_0:        #                     break        #                 x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    # sampling_type='ddim',        #                                    b=self.betas,        #                                    eta=1.0,        #                                    learn_sigma=learn_sigma,        #                                    )        #                 progress_bar.update(1)        #         print("Finish getting an ID latent encoding from sampling with p_s!", s)        #         coef_x1 = point_hyper(x_s.view(-1), x_lat.view(-1)).unsqueeze(0).numpy()        #         coef_h1 = point_hyper(h_s.view(-1), h_lat.view(-1)).unsqueeze(0).numpy()        #         save_x = './unseen_human_boundary/m2o_seed1006/human_coef_x'+ str(s)+'.npy'        #         save_h = './unseen_human_boundary/m2o_seed1006/human_coef_h'+ str(s)+'.npy'        #         np.save(save_x, coef_x1)        #         np.save(save_h, coef_h1)        #         print("Finish hyperplane search for OOD img:", s)                #get the         ####### One to Many Mapping        # ### prepare a group of OOD images, invert them to the same latent spaces as x_s and h_s        # with torch.no_grad():        #     # get an ID sample with stochastic process        #     seq_inv = np.linspace(0, 1, 999) * 999        #     seq_inv = [int(s) for s in list(seq_inv)]        #     seq_inv_next = [-1] + list(seq_inv[:-1])        #     x_s = torch.randn(1, 3, 256, 256, device=self.device)        #     with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:        #         for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #             t = (torch.ones(n) * i).to(self.device)        #             t_next = (torch.ones(n) * j).to(self.device)        #             # print("check step at generation:", t, t_next)        #             if t == self.args.t_0:        #                 break        #             x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                logvars=self.logvar,        #                                sampling_type=self.args.sample_type,        #                                # sampling_type='ddim',        #                                b=self.betas,        #                                eta=1.0,        #                                learn_sigma=learn_sigma,        #                                )        #             progress_bar.update(1)        #     print("Finish getting an ID latent encoding from sampling with p_s!", s)        # imgs = glob("/n/fs/yz-diff/BoundaryDiffusion/syn_imgs/train/human/pos/*")        # random.shuffle(imgs)        # # ood_x = torch.empty((len(imgs),3,256,256))        # # ood_h = torch.empty(len(imgs), 512, 8, 8)        # for c, im in enumerate(imgs):        #     img = Image.open(im).convert("RGB")        #     img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        #     img = np.array(img)/255        #     img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        #     img = img.to(self.config.device)        #     # img_name = str(c)+'_orig.png'        #     # tvu.save_image(img, os.path.join(self.args.image_folder, img_name))        #     x0 = (img - 0.5) * 2.        #     with torch.no_grad():        #         #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#        #         if self.args.deterministic_inv:        #             # x_lat_path = os.path.join(self.args.image_folder, f'x_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}.pth')        #             # h_lat_path = os.path.join(self.args.image_folder, f'h_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}.pth')        #             # if not os.path.exists(x_lat_path):        #             seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        #             seq_inv = [int(s) for s in list(seq_inv)]        #             seq_inv_next = [-1] + list(seq_inv[:-1])        #             x = x0.clone()        #             with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:        #                 for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):        #                     t = (torch.ones(n) * i).to(self.device)        #                     t_prev = (torch.ones(n) * j).to(self.device)        #                     x, mid_h_g = denoising_step(x, t=t, t_next=t_prev, models=model,        #                                        logvars=self.logvar,        #                                        sampling_type='ddim',        #                                        b=self.betas,        #                                        eta=0,        #                                        learn_sigma=learn_sigma,        #                                        ratio=0,        #                                        )        #                     progress_bar.update(1)        #                 x_lat = x.clone()        #                 h_lat = mid_h_g.clone()        #     print("Finish inversion for the given image to step:", self.args.t_0, x_lat.size(), h_lat.size(), c)                    #     # ood_x[c, :, :, :] = x_lat        #     # ood_h[c, :, :, :] = h_lat              #     # define the hyperplane, connecting x_s to x_lat as the normal direction.        #     ## this equals         #     # normal_direc = x_lat.reshape(1,-1) - x_s.reshape(1,-1)         #     # normal_v = normal_direc / torch.norm(normal_direc)        #     # print("check n:", normal_v.size(), torch.norm(normal_v), torch.norm(normal_direc))        #     coef_x1 = point_hyper(x_s.view(-1), x_lat.view(-1)).unsqueeze(0).numpy()        #     coef_h1 = point_hyper(h_s.view(-1), h_lat.view(-1)).unsqueeze(0).numpy()        #     # print("check hyper param:", coef_x1.size())        #     # print("check hyper param:", coef_h1.size())        #     # exit()        #     #### save the first hyperplane param as test ####         #     save_x = './unseen_human_boundary/o2m_seed1006/human_coef_x'+ str(c)+'.npy'        #     save_h = './unseen_human_boundary/o2m_seed1006/human_coef_h'+ str(c)+'.npy'        #     np.save(save_x, coef_x1)        #     np.save(save_h, coef_h1)        #     print("Finish hyperplane search for OOD img:", c)            # exit()        ###### Navigation Test Part #####            # coef_x1 = np.load('./unseen_human_boundary/o2m_seed1006/human_coef_x2.npy')        # coef_h1 = np.load('./unseen_human_boundary/o2m_seed1006/human_coef_h2.npy')        # print("First hyperplane loaded!")        # # ----- Editing space ------ #        # start_distance = self.args.start_distance          # end_distance = self.args.end_distance        # edit_img_number = self.args.edit_img_number        # print("check editing space:", start_distance, end_distance, edit_img_number)        # # exit()        # h_linspace = np.linspace(start_distance, end_distance, edit_img_number)        # h_latent_code = h_s.cpu().view(1,-1).numpy()        # h_linspace = h_linspace - h_latent_code.dot(coef_h1.T)        # h_linspace = h_linspace.reshape(-1, 1).astype(np.float32)        # edit_h_seq = h_latent_code + h_linspace * coef_h1        # z_linspace = np.linspace(start_distance, end_distance, edit_img_number)        # z_latent_code = x_s.cpu().view(1,-1).numpy()        # z_linspace = z_linspace - z_latent_code.dot(coef_x1.T)        # z_linspace = z_linspace.reshape(-1, 1).astype(np.float32)        # edit_z_seq = z_latent_code + z_linspace * coef_x1           # with torch.no_grad():        #     if self.args.n_test_step != 0:        #         seq_test = np.linspace(0, 1, self.args.n_test_step) * self.args.t_0        #         seq_test = [int(s) for s in list(seq_test)]        #         print('Uniform skip type')        #     else:        #         seq_test = list(range(self.args.t_0))        #         print('No skip')        #     seq_test_next = [-1] + list(seq_test[:-1])              #     for k in range(edit_img_number):        #         time_in_start = time.time()        #         with tqdm(total=len(seq_test), desc="Generative process {}".format(it)) as progress_bar:        #             edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)        #             edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)        #             for i, j in zip(reversed(seq_test), reversed(seq_test_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    eta = 0.05,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    edit_h=edit_h,        #                                    )        #         # x0 = x.clone()        #         save_edit = "unseen_traj"+str(k)+".png"        #         tvu.save_image((edit_z + 1) * 0.5, os.path.join("unseen_human_o2m_result",save_edit))        #         time_in_end = time.time()        #         print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")                        #     with tqdm(total=len(seq_test), desc="Generative process {}".format(it)) as progress_bar:        #         for i, j in zip(reversed(seq_test), reversed(seq_test_next)):        #             t = (torch.ones(n) * i).to(self.device)        #             t_next = (torch.ones(n) * j).to(self.device)        #             print("check eta:", self.args.eta)        #             x_s, _ = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                logvars=self.logvar,        #                                sampling_type=self.args.sample_type,        #                                b=self.betas,        #                                # eta=self.args.eta,        #                                eta = 0.0,        #                                learn_sigma=learn_sigma,        #                                ratio=self.args.model_ratio,        #                                hybrid=self.args.hybrid_noise,        #                                hybrid_config=HYBRID_CONFIG,        #                                edit_h=None,        #                                )        #             # added intermediate step vis        #             if (i - 99) % 100 == 0:        #                 tvu.save_image((x + 1) * 0.5, os.path.join(self.args.image_folder,        #                                                            f'2_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}_ngen{self.args.n_test_step}_{i}_it{it}.png'))        #             progress_bar.update(1)        #     # x0 = x.clone()        #     save_edit = "id_synthesis.png"        #     tvu.save_image((x_s + 1) * 0.5, os.path.join("unseen_human_o2m_result",save_edit))        # exit()                #### Navigation test part 2: Many to one        # coef_x1 = np.load('./unseen_human_boundary/m2o_seed1006/human_coef_x0.npy')        # coef_h1 = np.load('./unseen_human_boundary/m2o_seed1006/human_coef_h0.npy')        # print("First hyperplane loaded!")        # # get many ID samples with stochastic process        # seq_inv = np.linspace(0, 1, 999) * 999        # seq_inv = [int(s) for s in list(seq_inv)]        # seq_inv_next = [-1] + list(seq_inv[:-1])        # x_s_batch = torch.randn(50, 3, 256, 256, device=self.device)        # with torch.no_grad():        #     for s in range(50):        #         x_s = x_s_batch[5,:,:,:].unsqueeze(0)        #         with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:        #             for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 # print("check step at generation:", t, t_next)        #                 if t == self.args.t_0:        #                     break        #                 x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    # sampling_type='ddim',        #                                    b=self.betas,        #                                    eta=1.0,        #                                    learn_sigma=learn_sigma,        #                                    )        #                 progress_bar.update(1)        #         print("Finish getting an ID latent encoding from sampling with p_s!", s)        #         start_distance = self.args.start_distance          #         end_distance = self.args.end_distance        #         edit_img_number = self.args.edit_img_number        #         print("check editing space:", start_distance, end_distance, edit_img_number)        #         # exit()        #         h_linspace = np.linspace(start_distance, end_distance, edit_img_number)        #         h_latent_code = h_s.cpu().view(1,-1).numpy()        #         h_linspace = h_linspace - h_latent_code.dot(coef_h1.T)        #         h_linspace = h_linspace.reshape(-1, 1).astype(np.float32)        #         edit_h_seq = h_latent_code + h_linspace * coef_h1        #         z_linspace = np.linspace(start_distance, end_distance, edit_img_number)        #         z_latent_code = x_s.cpu().view(1,-1).numpy()        #         z_linspace = z_linspace - z_latent_code.dot(coef_x1.T)        #         z_linspace = z_linspace.reshape(-1, 1).astype(np.float32)        #         edit_z_seq = z_latent_code + z_linspace * coef_x1          #         if self.args.n_test_step != 0:        #             seq_test = np.linspace(0, 1, self.args.n_test_step) * self.args.t_0        #             seq_test = [int(s) for s in list(seq_test)]        #             print('Uniform skip type')        #         else:        #             seq_test = list(range(self.args.t_0))        #             print('No skip')        #         seq_test_next = [-1] + list(seq_test[:-1])              #         for k in range(edit_img_number):        #             time_in_start = time.time()        #             with tqdm(total=len(seq_test), desc="Generative process {}".format(it)) as progress_bar:        #                 edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)        #                 edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)        #                 for i, j in zip(reversed(seq_test), reversed(seq_test_next)):        #                     t = (torch.ones(n) * i).to(self.device)        #                     t_next = (torch.ones(n) * j).to(self.device)        #                     edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                        logvars=self.logvar,        #                                        sampling_type=self.args.sample_type,        #                                        b=self.betas,        #                                        eta = 0.05,        #                                        learn_sigma=learn_sigma,        #                                        ratio=self.args.model_ratio,        #                                        hybrid=self.args.hybrid_noise,        #                                        hybrid_config=HYBRID_CONFIG,        #                                        edit_h=edit_h,        #                                        )        #             # x0 = x.clone()        #             save_edit = "unseen_traj"+str(k)+".png"        #             tvu.save_image((edit_z + 1) * 0.5, os.path.join("unseen_human_m2o_result",save_edit))        #             time_in_end = time.time()        #             print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")        #         with tqdm(total=len(seq_test), desc="Generative process {}".format(it)) as progress_bar:        #             for i, j in zip(reversed(seq_test), reversed(seq_test_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 print("check eta:", self.args.eta)        #                 x_s, _ = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    # eta=self.args.eta,        #                                    eta = 0.0,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    edit_h=None,        #                                    )        #                 # added intermediate step vis        #                 if (i - 99) % 100 == 0:        #                     tvu.save_image((x + 1) * 0.5, os.path.join(self.args.image_folder,        #                                                                f'2_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}_ngen{self.args.n_test_step}_{i}_it{it}.png'))        #                 progress_bar.update(1)        #         # x0 = x.clone()        #         save_edit = "id_synthesis.png"        #         tvu.save_image((x_s + 1) * 0.5, os.path.join("unseen_human_m2o_result",save_edit))        #         exit()                ##### Navigation test part 3: dynamic gradual changing        # draw a random ID sample from Gaussian distribution as the departure latent encoding        seq_inv = np.linspace(0, 1, 999) * 999        seq_inv = [int(s) for s in list(seq_inv)]        seq_inv_next = [-1] + list(seq_inv[:-1])        # note this should be differnt from the seed used to get the hyperplanes (1006)        x_s_batch = torch.randn(100, 3, 256, 256, device=self.device)        with torch.no_grad():            x_s = x_s_batch[-1,:,:,:].unsqueeze(0)            with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:                for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):                    t = (torch.ones(n) * i).to(self.device)                    t_next = (torch.ones(n) * j).to(self.device)                    # print("check step at generation:", t, t_next)                    if t == self.args.t_0:                        break                    x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,                                       logvars=self.logvar,                                       sampling_type=self.args.sample_type,                                       # sampling_type='ddim',                                       b=self.betas,                                       eta=1.0,                                       learn_sigma=learn_sigma,                                       )                    progress_bar.update(1)            print("Finish getting an ID latent encoding from sampling with p_s at the mixing step!")                    ## now load the first boundary set from o2m            coef_x1 = np.load('./unseen_human_boundary/o2m_seed1006/human_coef_x0.npy')            coef_h1 = np.load('./unseen_human_boundary/o2m_seed1006/human_coef_h0.npy')            ## define the editing space            start_distance = self.args.start_distance              end_distance = self.args.end_distance            edit_img_number = self.args.edit_img_number            print("check editing space:", start_distance, end_distance, edit_img_number)            h_linspace = np.linspace(start_distance, end_distance, edit_img_number)            h_latent_code = h_s.cpu().view(1,-1).numpy()            h_linspace = h_linspace - h_latent_code.dot(coef_h1.T)            h_linspace = h_linspace.reshape(-1, 1).astype(np.float32)            edit_h_seq = h_latent_code + h_linspace * coef_h1            z_linspace = np.linspace(start_distance, end_distance, edit_img_number)            z_latent_code = x_s.cpu().view(1,-1).numpy()            z_linspace = z_linspace - z_latent_code.dot(coef_x1.T)            z_linspace = z_linspace.reshape(-1, 1).astype(np.float32)            edit_z_seq = z_latent_code + z_linspace * coef_x1             # exit()             if self.args.n_test_step != 0:                seq_test = np.linspace(0, 1, self.args.n_test_step) * self.args.t_0                seq_test = [int(s) for s in list(seq_test)]                print('Uniform skip type')            else:                seq_test = list(range(self.args.t_0))                print('No skip')            seq_test_next = [-1] + list(seq_test[:-1])                           # finish the first navigation, saving best (hard-coded), usually with the last one            h_s = torch.from_numpy(edit_h_seq[20]).to(self.device).view(-1, 512, 8, 8)            x_s = torch.from_numpy(edit_z_seq[20]).to(self.device).view(-1, 3, 256, 256)            save_edit = "nav1_latent.png"            tvu.save_image((x_s + 1) * 0.5, os.path.join("unseen_human_result",save_edit))            print("Finished the first navigation, latent encoding location updated:", x_s.size(), h_s.size())            ### start second navigation            coef_x1 = np.load('./unseen_human_boundary/o2m_seed1006/human_coef_x1.npy')            coef_h1 = np.load('./unseen_human_boundary/o2m_seed1006/human_coef_h1.npy')            ## define the editing space, may be reduced            start_distance = self.args.start_distance              end_distance = self.args.end_distance            edit_img_number = self.args.edit_img_number            print("check editing space:", start_distance, end_distance, edit_img_number)            h_linspace = np.linspace(start_distance, end_distance, edit_img_number)            h_latent_code = h_s.cpu().view(1,-1).numpy()            h_linspace = h_linspace - h_latent_code.dot(coef_h1.T)            h_linspace = h_linspace.reshape(-1, 1).astype(np.float32)            edit_h_seq = h_latent_code + h_linspace * coef_h1            z_linspace = np.linspace(start_distance, end_distance, edit_img_number)            z_latent_code = x_s.cpu().view(1,-1).numpy()            z_linspace = z_linspace - z_latent_code.dot(coef_x1.T)            z_linspace = z_linspace.reshape(-1, 1).astype(np.float32)            edit_z_seq = z_latent_code + z_linspace * coef_x1             # finish the first navigation, saving best (hard-coded), usually with the last one            x_h = torch.from_numpy(edit_h_seq[20]).to(self.device).view(-1, 512, 8, 8)            x_s = torch.from_numpy(edit_z_seq[20]).to(self.device).view(-1, 3, 256, 256)            save_edit = "nav2_latent.png"            tvu.save_image((x_s + 1) * 0.5, os.path.join("unseen_human_result",save_edit))            print("Finished the second navigation, latent encoding location updated:", x_s.size(), x_h.size())            ### start third navigation            coef_x1 = np.load('./unseen_human_boundary/o2m_seed1006/human_coef_x2.npy')            coef_h1 = np.load('./unseen_human_boundary/o2m_seed1006/human_coef_h2.npy')            ## define the editing space, may be reduced            start_distance = self.args.start_distance              end_distance = self.args.end_distance            edit_img_number = self.args.edit_img_number            print("check editing space:", start_distance, end_distance, edit_img_number)            h_linspace = np.linspace(start_distance, end_distance, edit_img_number)            h_latent_code = h_s.cpu().view(1,-1).numpy()            h_linspace = h_linspace - h_latent_code.dot(coef_h1.T)            h_linspace = h_linspace.reshape(-1, 1).astype(np.float32)            edit_h_seq = h_latent_code + h_linspace * coef_h1            z_linspace = np.linspace(start_distance, end_distance, edit_img_number)            z_latent_code = x_s.cpu().view(1,-1).numpy()            z_linspace = z_linspace - z_latent_code.dot(coef_x1.T)            z_linspace = z_linspace.reshape(-1, 1).astype(np.float32)            edit_z_seq = z_latent_code + z_linspace * coef_x1             for k in range(edit_img_number):                time_in_start = time.time()                with tqdm(total=len(seq_test), desc="Generative process {}".format(it)) as progress_bar:                    edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)                    edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)                    for i, j in zip(reversed(seq_test), reversed(seq_test_next)):                        t = (torch.ones(n) * i).to(self.device)                        t_next = (torch.ones(n) * j).to(self.device)                        edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,                                           logvars=self.logvar,                                           sampling_type=self.args.sample_type,                                           b=self.betas,                                           eta = 0.05,                                           learn_sigma=learn_sigma,                                           ratio=self.args.model_ratio,                                           hybrid=self.args.hybrid_noise,                                           hybrid_config=HYBRID_CONFIG,                                           edit_h=edit_h,                                           )                # x0 = x.clone()                save_edit = "unseen_traj_3_nav"+str(k)+".png"                tvu.save_image((edit_z + 1) * 0.5, os.path.join("unseen_human_result",save_edit))                time_in_end = time.time()                print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")              with tqdm(total=len(seq_test), desc="Generative process {}".format(it)) as progress_bar:                for i, j in zip(reversed(seq_test), reversed(seq_test_next)):                    t = (torch.ones(n) * i).to(self.device)                    t_next = (torch.ones(n) * j).to(self.device)                    # print("check eta:", self.args.eta)                    x_s, _ = denoising_step(x_s, t=t, t_next=t_next, models=model,                                       logvars=self.logvar,                                       sampling_type=self.args.sample_type,                                       b=self.betas,                                       # eta=self.args.eta,                                       eta = 0.05,                                       learn_sigma=learn_sigma,                                       ratio=self.args.model_ratio,                                       hybrid=self.args.hybrid_noise,                                       hybrid_config=HYBRID_CONFIG,                                       edit_h=None,                                       )                    progress_bar.update(1)            # x0 = x.clone()            save_edit = "id_synthesis_nav3.png"            tvu.save_image((x_s + 1) * 0.5, os.path.join("unseen_human_result",save_edit))            x_h = torch.from_numpy(edit_h_seq[20]).to(self.device).view(-1, 512, 8, 8)            x_s = torch.from_numpy(edit_z_seq[20]).to(self.device).view(-1, 3, 256, 256)            save_edit = "nav3_latent.png"            tvu.save_image((x_s + 1) * 0.5, os.path.join("unseen_human_result",save_edit))            print("Finished the third navigation, latent encoding location updated:", x_s.size(), x_h.size())            ### start fourth navigation            coef_x1 = np.load('./unseen_human_boundary/o2m_seed1006/human_coef_x3.npy')            coef_h1 = np.load('./unseen_human_boundary/o2m_seed1006/human_coef_h3.npy')            ## define the editing space, may be reduced            start_distance = self.args.start_distance              end_distance = self.args.end_distance            edit_img_number = self.args.edit_img_number            print("check editing space:", start_distance, end_distance, edit_img_number)            h_linspace = np.linspace(start_distance, end_distance, edit_img_number)            h_latent_code = h_s.cpu().view(1,-1).numpy()            h_linspace = h_linspace - h_latent_code.dot(coef_h1.T)            h_linspace = h_linspace.reshape(-1, 1).astype(np.float32)            edit_h_seq = h_latent_code + h_linspace * coef_h1            z_linspace = np.linspace(start_distance, end_distance, edit_img_number)            z_latent_code = x_s.cpu().view(1,-1).numpy()            z_linspace = z_linspace - z_latent_code.dot(coef_x1.T)            z_linspace = z_linspace.reshape(-1, 1).astype(np.float32)            edit_z_seq = z_latent_code + z_linspace * coef_x1             for k in range(edit_img_number):                time_in_start = time.time()                with tqdm(total=len(seq_test), desc="Generative process {}".format(it)) as progress_bar:                    edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)                    edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)                    for i, j in zip(reversed(seq_test), reversed(seq_test_next)):                        t = (torch.ones(n) * i).to(self.device)                        t_next = (torch.ones(n) * j).to(self.device)                        edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,                                           logvars=self.logvar,                                           sampling_type=self.args.sample_type,                                           b=self.betas,                                           eta = 0.05,                                           learn_sigma=learn_sigma,                                           ratio=self.args.model_ratio,                                           hybrid=self.args.hybrid_noise,                                           hybrid_config=HYBRID_CONFIG,                                           edit_h=edit_h,                                           )                # x0 = x.clone()                save_edit = "unseen_traj_4_nav"+str(k)+".png"                tvu.save_image((edit_z + 1) * 0.5, os.path.join("unseen_human_result",save_edit))                time_in_end = time.time()                print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")              with tqdm(total=len(seq_test), desc="Generative process {}".format(it)) as progress_bar:                for i, j in zip(reversed(seq_test), reversed(seq_test_next)):                    t = (torch.ones(n) * i).to(self.device)                    t_next = (torch.ones(n) * j).to(self.device)                    # print("check eta:", self.args.eta)                    x_s, _ = denoising_step(x_s, t=t, t_next=t_next, models=model,                                       logvars=self.logvar,                                       sampling_type=self.args.sample_type,                                       b=self.betas,                                       # eta=self.args.eta,                                       eta = 0.05,                                       learn_sigma=learn_sigma,                                       ratio=self.args.model_ratio,                                       hybrid=self.args.hybrid_noise,                                       hybrid_config=HYBRID_CONFIG,                                       edit_h=None,                                       )                    progress_bar.update(1)            # x0 = x.clone()            save_edit = "id_synthesis_nav4.png"            tvu.save_image((x_s + 1) * 0.5, os.path.join("unseen_human_result",save_edit))            x_h = torch.from_numpy(edit_h_seq[20]).to(self.device).view(-1, 512, 8, 8)            x_s = torch.from_numpy(edit_z_seq[20]).to(self.device).view(-1, 3, 256, 256)            save_edit = "nav4_latent.png"            tvu.save_image((x_s + 1) * 0.5, os.path.join("unseen_human_result",save_edit))            print("Finished the fourth navigation, latent encoding location updated:", x_s.size(), x_h.size())        return None                def test_func(self):        print(self.args.exp)        # ----------- Model -----------#        if self.config.data.dataset == "LSUN":            if self.config.data.category == "bedroom":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt"            elif self.config.data.category == "church_outdoor":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt"        elif self.config.data.dataset == "CelebA_HQ":            url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt"        elif self.config.data.dataset == "AFHQ":            pass        else:            raise ValueError        if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:            model = DDPM(self.config)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)            learn_sigma = False            print("Original diffusion Model loaded.")        elif self.config.data.dataset in ["FFHQ", "AFHQ"]:            model = i_DDPM(self.config.data.dataset)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])            learn_sigma = True            print("Improved diffusion Model loaded.")        else:            print('Not implemented dataset')            raise ValueError        model.load_state_dict(init_ckpt)        model.to(self.device)        model = torch.nn.DataParallel(model)        model.eval()            n = self.args.bs_test        ### get the inverted OOD latent encodings        # #### Test 1, separability check        # origin = torch.zeros(1, 3, 256, 256, device=self.device)        # imgs = glob("/n/fs/yz-diff/BoundaryDiffusion/syn_imgs/train/human/pos/*")        # # random.shuffle(imgs)        # imgs = sorted(imgs)        # ood_x = torch.empty((len(imgs),3,256,256))        # ood_h = torch.empty(len(imgs), 512, 8, 8)        # r_list = []        # r_ood = 0        # for c, im in enumerate(imgs):        #     img = Image.open(im).convert("RGB")        #     img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        #     img = np.array(img)/255        #     img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        #     img = img.to(self.config.device)        #     # img_name = str(c)+'_orig.png'        #     # tvu.save_image(img, "ood_origin.png")        #     x0 = (img - 0.5) * 2.        #     with torch.no_grad():        #         #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#        #         if self.args.deterministic_inv:        #             # x_lat_path = os.path.join(self.args.image_folder, f'x_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}.pth')        #             # h_lat_path = os.path.join(self.args.image_folder, f'h_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}.pth')        #             # if not os.path.exists(x_lat_path):        #             seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        #             seq_inv = [int(s) for s in list(seq_inv)]        #             seq_inv_next = [-1] + list(seq_inv[:-1])        #             # seq_inv = np.linspace(0, 1, 999) * 999        #             # seq_inv = [int(s) for s in list(seq_inv)]        #             # seq_inv_next = [-1] + list(seq_inv[:-1])        #             x = x0.clone()        #             with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:        #                 for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):        #                     t = (torch.ones(n) * i).to(self.device)        #                     t_prev = (torch.ones(n) * j).to(self.device)        #                     x, mid_h_g = denoising_step(x, t=t, t_next=t_prev, models=model,        #                                        logvars=self.logvar,        #                                        sampling_type='ddim',        #                                        b=self.betas,        #                                        eta=0,        #                                        learn_sigma=learn_sigma,        #                                        ratio=0,        #                                        )        #                     progress_bar.update(1)        #                 x_lat = x.clone()        #                 h_lat = mid_h_g.clone()                # with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:                #     for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):                #         t = (torch.ones(n) * i).to(self.device)                #         t_next = (torch.ones(n) * j).to(self.device)                #         print("check eta:", self.args.eta)                #         x, _ = denoising_step(x, t=t, t_next=t_next, models=model,                #                            logvars=self.logvar,                #                            sampling_type=self.args.sample_type,                #                            b=self.betas,                #                            eta = self.args.eta,                #                            learn_sigma=learn_sigma,                #                            ratio=self.args.model_ratio,                #                            hybrid=self.args.hybrid_noise,                #                            hybrid_config=HYBRID_CONFIG,                #                            )                # tvu.save_image((x + 1) * 0.5, os.path.join("ood_recons_900.png"))                # exit()        #     print("Finish inversion for the given image to step:", c, self.args.t_0, x_lat.size(), h_lat.size(), c)        #     radius_origin1 = compute_radius(x_lat)        #     radius_origin2 = euclidean_distance(origin, x_lat)        #     r_list.append(radius_origin2)        #     r_ood += radius_origin1        #     print("check radius:", radius_origin1, radius_origin2)        #     # exit()        #     ood_x[c, :, :, :] = x_lat        #     ood_h[c, :, :, :] = h_lat         #     # break          # r_ood = r_ood/len(imgs)        # print("check radius to origin:", r_ood, torch.mean(torch.tensor(r_list)), torch.std(torch.tensor(r_list)))        # # local_center = torch.mean()        # local_center_x = torch.mean(ood_x, dim=0).unsqueeze(0)        # local_center_h = torch.mean(ood_h, dim=0).unsqueeze(0)        # print("check center form:", local_center_x.size(), local_center_h.size())        # torch.save(local_center_x,"human_center_x600.pth")        # torch.save(local_center_h,"human_center_h600.pth")        # exit()        # r_ood_local = []        # for i in range(len(r_list)):        #     # print("check radius to local center:", euclidean_distance(ood_x[i,:,:,:],local_center))        #     r_ood_local.append(euclidean_distance(ood_x[i,:,:,:],local_center))        # print("check radius to local center:", torch.mean(torch.tensor(r_ood_local)), torch.std(torch.tensor(r_ood_local)))        # print("check distance between origin and local_center:", euclidean_distance(origin, local_center.to(self.device)))        # exit()        # ###### Test 2.1 Convex check        # # get any two OOD samples and do the interpolation, check t_m = 600-500, eta = 0 - 0.3        # imgs = glob("/n/fs/yz-diff/BoundaryDiffusion/syn_imgs/train/human/pos/*")        # imgs = sorted(imgs)        # img = Image.open(imgs[4]).convert("RGB")        # img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        # img = np.array(img)/255        # img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        # img = img.to(self.config.device)        # tvu.save_image(img, "ood_convex0.png")        # x0 = (img - 0.5) * 2.        # with torch.no_grad():        #     #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#        #     if self.args.deterministic_inv:        #         seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        #         seq_inv = [int(s) for s in list(seq_inv)]        #         seq_inv_next = [-1] + list(seq_inv[:-1])        #         x = x0.clone()        #         with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:        #             for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_prev = (torch.ones(n) * j).to(self.device)        #                 x, mid_h_g = denoising_step(x, t=t, t_next=t_prev, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type='ddim',        #                                    b=self.betas,        #                                    eta=0,        #                                    learn_sigma=learn_sigma,        #                                    ratio=0,        #                                    )        #                 progress_bar.update(1)        #             x_1 = x.clone()        #             h_1 = mid_h_g.clone()        # img = Image.open(imgs[5]).convert("RGB")        # img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        # img = np.array(img)/255        # img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        # img = img.to(self.config.device)        # tvu.save_image(img, "ood_convex1.png")        # x0 = (img - 0.5) * 2.        # with torch.no_grad():        #     #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#        #     if self.args.deterministic_inv:        #         seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        #         seq_inv = [int(s) for s in list(seq_inv)]        #         seq_inv_next = [-1] + list(seq_inv[:-1])        #         x = x0.clone()        #         with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:        #             for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_prev = (torch.ones(n) * j).to(self.device)        #                 x, mid_h_g = denoising_step(x, t=t, t_next=t_prev, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type='ddim',        #                                    b=self.betas,        #                                    eta=0,        #                                    learn_sigma=learn_sigma,        #                                    ratio=0,        #                                    )        #                 progress_bar.update(1)        #             x_2 = x.clone()        #             h_2 = mid_h_g.clone()        # ##-- do the interpolaton         # interpolated_seq = lin_interpolation(x_1, x_2, 10)        # print("check size:", interpolated_seq.size())        # with torch.no_grad():        #     for k in range(10):        #         x = interpolated_seq[k,:,:,:].to(self.device)        #         with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:        #             for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 # print("check eta:", self.args.eta)        #                 x, _ = denoising_step(x, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    eta = self.args.eta,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    )        #         save_path = "convex_test"+str(k)+".png"        #         tvu.save_image((x + 1) * 0.5, os.path.join("convex_test",save_path))        ### Test 2.1 - subspace capture using hyperplanes - local center or origin        # origin_x = torch.zeros(1, 3, 256, 256, device=self.device)        # origin_h = torch.zeros(1, 512, 8, 8, device=self.device)        # local_centor_x = torch.load('./test2_center/human_center_x400.pth').to(self.device)        # local_center_h = torch.load('./test2_center/human_center_h400.pth').to(self.device)        # imgs = glob("/n/fs/yz-diff/BoundaryDiffusion/syn_imgs/train/human/pos/*")        # # random.shuffle(imgs)        # imgs = sorted(imgs)        # ood_x = torch.empty((len(imgs),3,256,256))        # ood_h = torch.empty(len(imgs), 512, 8, 8)        # for c, im in enumerate(imgs):        #     img = Image.open(im).convert("RGB")        #     img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        #     img = np.array(img)/255        #     img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        #     img = img.to(self.config.device)        #     # img_name = str(c)+'_orig.png'        #     # tvu.save_image(img, "ood_origin.png")        #     x0 = (img - 0.5) * 2.        #     with torch.no_grad():        #         #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#        #         if self.args.deterministic_inv:        #             # x_lat_path = os.path.join(self.args.image_folder, f'x_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}.pth')        #             # h_lat_path = os.path.join(self.args.image_folder, f'h_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}.pth')        #             # if not os.path.exists(x_lat_path):        #             seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        #             seq_inv = [int(s) for s in list(seq_inv)]        #             seq_inv_next = [-1] + list(seq_inv[:-1])        #             # seq_inv = np.linspace(0, 1, 999) * 999        #             # seq_inv = [int(s) for s in list(seq_inv)]        #             # seq_inv_next = [-1] + list(seq_inv[:-1])        #             x = x0.clone()        #             with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:        #                 for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):        #                     t = (torch.ones(n) * i).to(self.device)        #                     t_prev = (torch.ones(n) * j).to(self.device)        #                     x, mid_h_g = denoising_step(x, t=t, t_next=t_prev, models=model,        #                                        logvars=self.logvar,        #                                        sampling_type='ddim',        #                                        b=self.betas,        #                                        eta=0,        #                                        learn_sigma=learn_sigma,        #                                        ratio=0,        #                                        )        #                     progress_bar.update(1)        #                 x_lat = x.clone()        #                 h_lat = mid_h_g.clone()        #                 coef_x1 = point_hyper(local_centor_x.view(-1), x_lat.view(-1)).unsqueeze(0).numpy()        #                 coef_h1 = point_hyper(local_center_h.view(-1), h_lat.view(-1)).unsqueeze(0).numpy()        #                 save_x = './test2_center/subspace_400/human_coef_x'+ str(c)+'.npy'        #                 save_h = './test2_center/subspace_400/human_coef_h'+ str(c)+'.npy'        #                 np.save(save_x, coef_x1)        #                 np.save(save_h, coef_h1)        #                 print("Finish hyperplane search for OOD img:", c)        #### Test 3: Navigation algo        # First load all the subspace hyperplanes        seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        seq_inv = [int(s) for s in list(seq_inv)]        seq_inv_next = [-1] + list(seq_inv[:-1])               # note this should be differnt from the seed used to get the hyperplanes (1006)        x_s_batch = torch.randn(100, 3, 256, 256, device=self.device)        with torch.no_grad():            x_s = x_s_batch[1,:,:,:].unsqueeze(0)            with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:                for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):                    t = (torch.ones(n) * i).to(self.device)                    t_next = (torch.ones(n) * j).to(self.device)                    x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,                                       logvars=self.logvar,                                       sampling_type=self.args.sample_type,                                       # sampling_type='ddim',                                       b=self.betas,                                       eta=1.0,                                       learn_sigma=learn_sigma,                                       )                    progress_bar.update(1)            print("Finish getting an ID latent encoding from sampling with p_s at the mixing step!")              # boudaries_x = glob.glob("./test2_center/subspace_500/*")            coef_x1 = np.load('./test2_center/subspace_500/human_coef_x0.npy')            coef_h1 = np.load('./test2_center/subspace_500/human_coef_h0.npy')            edit_h_seq, edit_z_seq, h_s, x_s = navigation_iter(-300, 300,  30, 20, coef_x1, coef_h1, h_s, x_s)        # print("check edit_h_seq and edit_z_seq:", np.shape(edit_h_seq), np.shape(edit_z_seq))        with torch.no_grad():            for k in range(len(edit_z_seq)):                time_in_start = time.time()                with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:                    edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)                    edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)                    for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):                        t = (torch.ones(n) * i).to(self.device)                        t_next = (torch.ones(n) * j).to(self.device)                        edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,                                           logvars=self.logvar,                                           sampling_type=self.args.sample_type,                                           b=self.betas,                                           eta = self.args.eta,                                           learn_sigma=learn_sigma,                                           ratio=self.args.model_ratio,                                           hybrid=self.args.hybrid_noise,                                           hybrid_config=HYBRID_CONFIG,                                           edit_h=edit_h,                                           )                # x0 = x.clone()                save_edit = "unseen_traj1_nav"+str(k)+".png"                tvu.save_image((edit_z + 1) * 0.5, os.path.join("test3_result",save_edit))                time_in_end = time.time()                print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")          ### update the h_s and x_s and hyperplans        # h_s = edit_h_seq[20]        # x_s = edit_z_seq[20]        print("Begin second navigation!")        coef_x1 = np.load('./test2_center/subspace_500/human_coef_x1.npy')        coef_h1 = np.load('./test2_center/subspace_500/human_coef_h1.npy')             edit_h_seq, edit_z_seq, h_s, x_s = navigation_iter(-300, 300,  30, 20, coef_x1, coef_h1, h_s, x_s)         with torch.no_grad():            for k in range(len(edit_z_seq)):                time_in_start = time.time()                with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:                    edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)                    edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)                    for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):                        t = (torch.ones(n) * i).to(self.device)                        t_next = (torch.ones(n) * j).to(self.device)                        edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,                                           logvars=self.logvar,                                           sampling_type=self.args.sample_type,                                           b=self.betas,                                           eta = self.args.eta,                                           learn_sigma=learn_sigma,                                           ratio=self.args.model_ratio,                                           hybrid=self.args.hybrid_noise,                                           hybrid_config=HYBRID_CONFIG,                                           edit_h=edit_h,                                           )                # x0 = x.clone()                save_edit = "unseen_traj2_nav"+str(k)+".png"                tvu.save_image((edit_z + 1) * 0.5, os.path.join("test3_result",save_edit))                time_in_end = time.time()                print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")          print("Begin third navigation!")        coef_x1 = np.load('./test2_center/subspace_500/human_coef_x2.npy')        coef_h1 = np.load('./test2_center/subspace_500/human_coef_h2.npy')             edit_h_seq, edit_z_seq, h_s, x_s = navigation_iter(-300, 300,  30, 20, coef_x1, coef_h1, h_s, x_s)         with torch.no_grad():            for k in range(len(edit_z_seq)):                time_in_start = time.time()                with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:                    edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)                    edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)                    for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):                        t = (torch.ones(n) * i).to(self.device)                        t_next = (torch.ones(n) * j).to(self.device)                        edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,                                           logvars=self.logvar,                                           sampling_type=self.args.sample_type,                                           b=self.betas,                                           eta = self.args.eta,                                           learn_sigma=learn_sigma,                                           ratio=self.args.model_ratio,                                           hybrid=self.args.hybrid_noise,                                           hybrid_config=HYBRID_CONFIG,                                           edit_h=edit_h,                                           )                # x0 = x.clone()                save_edit = "unseen_traj3_nav"+str(k)+".png"                tvu.save_image((edit_z + 1) * 0.5, os.path.join("test3_result",save_edit))                time_in_end = time.time()                print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")          print("-------Begin fourth navigation!")        coef_x1 = np.load('./test2_center/subspace_500/human_coef_x3.npy')        coef_h1 = np.load('./test2_center/subspace_500/human_coef_h3.npy')             edit_h_seq, edit_z_seq, h_s, x_s = navigation_iter(-300, 300,  30, 20, coef_x1, coef_h1, h_s, x_s)         with torch.no_grad():            for k in range(len(edit_z_seq)):                time_in_start = time.time()                with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:                    edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)                    edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)                    for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):                        t = (torch.ones(n) * i).to(self.device)                        t_next = (torch.ones(n) * j).to(self.device)                        edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,                                           logvars=self.logvar,                                           sampling_type=self.args.sample_type,                                           b=self.betas,                                           eta = self.args.eta,                                           learn_sigma=learn_sigma,                                           ratio=self.args.model_ratio,                                           hybrid=self.args.hybrid_noise,                                           hybrid_config=HYBRID_CONFIG,                                           edit_h=edit_h,                                           )                # x0 = x.clone()                save_edit = "unseen_traj4_nav"+str(k)+".png"                tvu.save_image((edit_z + 1) * 0.5, os.path.join("test3_result",save_edit))                time_in_end = time.time()                print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")          print("----------Begin fifth navigation!")        coef_x1 = np.load('./test2_center/subspace_500/human_coef_x4.npy')        coef_h1 = np.load('./test2_center/subspace_500/human_coef_h4.npy')             edit_h_seq, edit_z_seq, h_s, x_s = navigation_iter(-300, 300,  30, 20, coef_x1, coef_h1, h_s, x_s)         with torch.no_grad():            for k in range(len(edit_z_seq)):                time_in_start = time.time()                with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:                    edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)                    edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)                    for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):                        t = (torch.ones(n) * i).to(self.device)                        t_next = (torch.ones(n) * j).to(self.device)                        edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,                                           logvars=self.logvar,                                           sampling_type=self.args.sample_type,                                           b=self.betas,                                           eta = self.args.eta,                                           learn_sigma=learn_sigma,                                           ratio=self.args.model_ratio,                                           hybrid=self.args.hybrid_noise,                                           hybrid_config=HYBRID_CONFIG,                                           edit_h=edit_h,                                           )                # x0 = x.clone()                save_edit = "unseen_traj5_nav"+str(k)+".png"                tvu.save_image((edit_z + 1) * 0.5, os.path.join("test3_result",save_edit))                time_in_end = time.time()                print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")        print("Begin sixth navigation!")        coef_x1 = np.load('./test2_center/subspace_500/human_coef_x5.npy')        coef_h1 = np.load('./test2_center/subspace_500/human_coef_h5.npy')             edit_h_seq, edit_z_seq, h_s, x_s = navigation_iter(-300, 300,  30, 20, coef_x1, coef_h1, h_s, x_s)         with torch.no_grad():            for k in range(len(edit_z_seq)):                time_in_start = time.time()                with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:                    edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)                    edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)                    for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):                        t = (torch.ones(n) * i).to(self.device)                        t_next = (torch.ones(n) * j).to(self.device)                        edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,                                           logvars=self.logvar,                                           sampling_type=self.args.sample_type,                                           b=self.betas,                                           eta = self.args.eta,                                           learn_sigma=learn_sigma,                                           ratio=self.args.model_ratio,                                           hybrid=self.args.hybrid_noise,                                           hybrid_config=HYBRID_CONFIG,                                           edit_h=edit_h,                                           )                # x0 = x.clone()                save_edit = "unseen_traj6_nav"+str(k)+".png"                tvu.save_image((edit_z + 1) * 0.5, os.path.join("test3_result",save_edit))                time_in_end = time.time()                print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")        exit()    def navigation(self):        print(self.args.exp)        # ----------- Model -----------#        if self.config.data.dataset == "LSUN":            if self.config.data.category == "bedroom":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/bedroom.ckpt"            elif self.config.data.category == "church_outdoor":                url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/church_outdoor.ckpt"        elif self.config.data.dataset == "CelebA_HQ":            url = "https://image-editing-test-12345.s3-us-west-2.amazonaws.com/checkpoints/celeba_hq.ckpt"        elif self.config.data.dataset == "AFHQ":            pass        else:            raise ValueError        if self.config.data.dataset in ["CelebA_HQ", "LSUN"]:            model = DDPM(self.config)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.hub.load_state_dict_from_url(url, map_location=self.device)            learn_sigma = False            print("Original diffusion Model loaded.")        elif self.config.data.dataset in ["FFHQ", "AFHQ"]:            model = i_DDPM(self.config.data.dataset)            if self.args.model_path:                init_ckpt = torch.load(self.args.model_path)            else:                init_ckpt = torch.load(MODEL_PATHS[self.config.data.dataset])            learn_sigma = True            print("Improved diffusion Model loaded.")        else:            print('Not implemented dataset')            raise ValueError        model.load_state_dict(init_ckpt)        model.to(self.device)        model = torch.nn.DataParallel(model)        model.eval()            n = self.args.bs_test        seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        seq_inv = [int(s) for s in list(seq_inv)]        seq_inv_next = [-1] + list(seq_inv[:-1])         origin = torch.zeros(1, 3, 256, 256, device=self.device)        x_s_batch = torch.randn(1000, 3, 256, 256, device=self.device)        # x_lat = x_s_batch[36,:,:,:].unsqueeze(0)        # ood_x = torch.load("./ood_samples/iddpm_dog_unseen_church_t500_x.pth").to(self.device)        # ood_1 = ood_x[1,:,:,:].unsqueeze(0)        # ood_2 = ood_x[5,:,:,:].unsqueeze(0)        # ood_3 = ood_x[9,:,:,:].unsqueeze(0)        id_x = torch.empty(100,3,256,256)        #######################################################################        for k in range(100):            x_lat = x_s_batch[660+k,:,:,:].unsqueeze(0)            with torch.no_grad():                with tqdm(total=len(seq_inv), desc="Generative process") as progress_bar:                    for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):                        t = (torch.ones(n) * i).to(self.device)                        t_next = (torch.ones(n) * j).to(self.device)                        x_lat, _ = denoising_step(x_lat, t=t, t_next=t_next, models=model,                                            logvars=self.logvar,                                            sampling_type=self.args.sample_type,                                            b=self.betas,                                            eta=self.args.eta,                                            learn_sigma=learn_sigma,                                            ratio=self.args.model_ratio,                                            hybrid=self.args.hybrid_noise,                                            hybrid_config=HYBRID_CONFIG,                                            edit_h=None,                                            )                        progress_bar.update(1)                x0 = x_lat.clone()        #     # save_edit = "id_synthesis_nav1.png"        #     # img_name = str(c)+'_recons.png'            # img_name = "recons_cat_"+str(self.args.t_0)+str(self.args.eta)+".png"                img_name = str(k) + ".png"                tvu.save_image((x0 + 1) * 0.5, os.path.join("/n/fs/yz-diff/BoundaryDiffusion/id_sampling_results/bedroom/",img_name))            # tvu.save_image((x0 + 1) * 0.5, img_name)         exit()                   im = "./cat.jpg"        # im = "./79.png"        img = Image.open(im).convert("RGB")        img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        img = np.array(img)/255        img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        img = img.to(self.config.device)        x0 = (img - 0.5) * 2        with torch.no_grad():            #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#            if self.args.deterministic_inv:                x = x0.clone()                with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:                    for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):                        t = (torch.ones(n) * i).to(self.device)                        t_prev = (torch.ones(n) * j).to(self.device)                        x, _ = denoising_step(x, t=t, t_next=t_prev, models=model,                                           logvars=self.logvar,                                           sampling_type='ddim',                                           b=self.betas,                                           eta=0,                                           learn_sigma=learn_sigma,                                           ratio=0,                                           )                        progress_bar.update(1)                    cat1 = x.clone()        x_lat = cat1        with torch.no_grad():            with tqdm(total=len(seq_inv), desc="Generative process") as progress_bar:                for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):                    t = (torch.ones(n) * i).to(self.device)                    t_next = (torch.ones(n) * j).to(self.device)                    x_lat, _ = denoising_step(x_lat, t=t, t_next=t_next, models=model,                                       logvars=self.logvar,                                       sampling_type=self.args.sample_type,                                       b=self.betas,                                       eta=self.args.eta,                                       learn_sigma=learn_sigma,                                       ratio=self.args.model_ratio,                                       hybrid=self.args.hybrid_noise,                                       hybrid_config=HYBRID_CONFIG,                                       edit_h=None,                                       )                    progress_bar.update(1)            x0 = x_lat.clone()        #     # save_edit = "id_synthesis_nav1.png"        #     # img_name = str(c)+'_recons.png'            img_name = "recons_cat_"+str(self.args.t_0)+str(self.args.eta)+".png"        #     # tvu.save_image((x0 + 1) * 0.5, os.path.join("/n/fs/yz-diff/BoundaryDiffusion/reconstruction_test/recons_ddpm_celeb_unseen_church/recons/",img_name_recons))            tvu.save_image((x0 + 1) * 0.5, img_name)             exit()              # ###################### Truncation range ##########################        # # #### test w/ flatten Standard Gaussian        # # samples = torch.randn(100, 3, 256, 256) #, device=self.device)        # samples = torch.randn(5, 3* 256* 256)        # mean, cov, truncation_range = fit_truncated_gaussian(samples)        # print(mean, cov, truncation_range)        # x_s_batch_faltten = x_s_batch.view(1000,-1)        # mean1 = torch.mean(x_s_batch, dim=0)        # std1 = torch.std(x_s_batch, dim=0)        # print("check 1:", x_s_batch.size(), torch.sum(mean1), torch.sum(std1))        # mean2 = torch.mean(x_s_batch_faltten)        # std2 = torch.std(x_s_batch_faltten)        # print("check 2:", x_s_batch_faltten.size(), mean2, std2)        #### truncation range, test w/ sampled standard Gaussian        # trunc, mean, cov = truncation(x_s_batch)        # print("check truncation:", trunc, mean, cov, cov.size())        # mean = torch.mean(samples, dim = 0)#.unsqueeze(0)        # print("check mean size:", (samples - mean).transpose(0, 1).size())        # covariance_matrix = torch.matmul((samples - mean).transpose(0, 1), samples - mean) #/ (samples.shape[0] - 1)        # max_distance = torch.max(torch.norm(samples - mean, dim=1))        # truncation_range = max_distance * 3        # fitted_gaussian = multivariate_normal(mean=mean.numpy(), cov=covariance_matrix.numpy())        # new_samples = []        # for i in range(10):        #     sample = torch.from_numpy(fitted_gaussian.rvs())        #     new_samples.append(sample)        #     new_samples = torch.stack(new_samples, dim=0)        # print("check new samples:", new_samples.size())        # exit()        ############## 0303 Qualitative #################        # x_s_batch = torch.randn(100, 3, 256, 256, device=self.device)        # x_lat = x_s_batch[68,:,:,:].unsqueeze(0)        # with torch.no_grad():        #     with tqdm(total=len(seq_inv), desc="Generative process") as progress_bar:        #         for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #             t = (torch.ones(n) * i).to(self.device)        #             t_next = (torch.ones(n) * j).to(self.device)        #             x_lat, _ = denoising_step(x_lat, t=t, t_next=t_next, models=model,        #                                logvars=self.logvar,        #                                sampling_type=self.args.sample_type,        #                                b=self.betas,        #                                eta=self.args.eta,        #                                learn_sigma=learn_sigma,        #                                ratio=self.args.model_ratio,        #                                hybrid=self.args.hybrid_noise,        #                                hybrid_config=HYBRID_CONFIG,        #                                edit_h=None,        #                                )        #             progress_bar.update(1)        #     x0 = x_lat.clone()              #     img_name = "id_sample5.png"        #     tvu.save_image((x0 + 1) * 0.5, img_name)        # im = "./church.png"        # im = "./celeb1.jpg"        # im = "./bedroom.png"        # im = "./cat.jpg"        # img = Image.open(im).convert("RGB")        # img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        # img = np.array(img)/255        # img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        # img = img.to(self.config.device)        # x0 = (img - 0.5) * 2        # with torch.no_grad():        #     #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#        #     if self.args.deterministic_inv:        #         x = x0.clone()        #         with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:        #             for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_prev = (torch.ones(n) * j).to(self.device)        #                 x, _ = denoising_step(x, t=t, t_next=t_prev, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type='ddim',        #                                    b=self.betas,        #                                    eta=0,        #                                    learn_sigma=learn_sigma,        #                                    ratio=0,        #                                    )        #                 progress_bar.update(1)        #             x_lat = x.clone()        #     # img_name = "celeb_latent"+str()+"".png"        #     # tvu.save_image((x_lat + 1) * 0.5, img_name)                    # with torch.no_grad():        #     with tqdm(total=len(seq_inv), desc="Generative process") as progress_bar:        #         for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #             t = (torch.ones(n) * i).to(self.device)        #             t_next = (torch.ones(n) * j).to(self.device)        #             x_lat, _ = denoising_step(x_lat, t=t, t_next=t_next, models=model,        #                                logvars=self.logvar,        #                                sampling_type=self.args.sample_type,        #                                b=self.betas,        #                                eta=self.args.eta,        #                                learn_sigma=learn_sigma,        #                                ratio=self.args.model_ratio,        #                                hybrid=self.args.hybrid_noise,        #                                hybrid_config=HYBRID_CONFIG,        #                                edit_h=None,        #                                )        #             progress_bar.update(1)        #     x0 = x_lat.clone()        # #     # save_edit = "id_synthesis_nav1.png"        # #     # img_name = str(c)+'_recons.png'        #     img_name = "recons_bandwidth"+str(self.args.eta)+".png"        # #     # tvu.save_image((x0 + 1) * 0.5, os.path.join("/n/fs/yz-diff/BoundaryDiffusion/reconstruction_test/recons_ddpm_celeb_unseen_church/recons/",img_name_recons))        #     tvu.save_image((x0 + 1) * 0.5, img_name)        # ###### 0228 Fitting a high-dim Gaussian        # imgs = glob("/n/fs/visualai-scr/Data/CelebA-HQ/raw_images/train/images/*")        # # imgs = glob("/n/fs/visualai-scr/Data/AFHQ-Dog/afhq/train/dog/*")        # # random.shuffle(imgs)        # # ood_x = torch.empty((len(imgs),3,256,256))        # # ood_h = torch.empty(len(imgs), 512, 8, 8)        # print("total OOD sample nums:", len(imgs))        # ood_x = []        # mu = []        # std = []        # for c, im in enumerate(imgs):        #     img = Image.open(im).convert("RGB")        #     img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        #     img = np.array(img)/255        #     img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        #     img = img.to(self.config.device)        #     x0 = (img - 0.5) * 2.        #     with torch.no_grad():        #         #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#        #         if self.args.deterministic_inv:        #             x = x0.clone()        #             with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:        #                 for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):        #                     t = (torch.ones(n) * i).to(self.device)        #                     t_prev = (torch.ones(n) * j).to(self.device)        #                     x, mid_h_g = denoising_step(x, t=t, t_next=t_prev, models=model,        #                                        logvars=self.logvar,        #                                        sampling_type='ddim',        #                                        b=self.betas,        #                                        eta=0,        #                                        learn_sigma=learn_sigma,        #                                        ratio=0,        #                                        )        #                     progress_bar.update(1)        #                 x_lat = x.clone()        #                 ood_x.append(x_lat.reshape(-1))        #     # print("Finish inversion for the given image to step:", self.args.t_0, x_lat.size(), c)        #     if (c+1) % 100 == 0:        #         ood_x = torch.stack(ood_x)        #         mu_ = torch.mean(ood_x)        #         std_ = torch.std(ood_x)        #         mu.append(mu_)        #         std.append(std_)        #         ood_save_name = "iddpm_ood_celeba_"+ str(c+1)+".pth"        #         print("check intermediate mu and std at:", (c+1), mu_, std_, ood_x.size())        #         torch.save(ood_x, ood_save_name)        #         ood_x = []        #         torch.save(mu, "iddpm_in_mu.pth")        #         torch.save(std, "iddpm_in_std.pth")                # break        ############### Fitting a Gaussian        # ood_x = torch.load("./ood_samples/iddpm_dog_unseen_celeba_t500_x.pth").to(self.device)        # ood_x_cat = torch.load("./ood_samples/iddpm_dog_unseen_church_t500_x.pth").to(self.device)        # num = ood_x.size()[0]        # # estimating the mean mu        # mu = torch.mean(ood_x, dim=0).unsqueeze(0)        # # mu_ = torch.sum(ood_x, dim=0) / num        # # print("check mu:", mu.size(), mu)        # # estimating the variance        # mu_ = torch.sum(ood_x, dim=0).unsqueeze(0) / (num-1)        # # print("check mu_:", mu_.size())        # sigma = 0        # for i in range(num):        #     sigma_ = torch.pow((ood_x[i,:,:,:].unsqueeze(0) - mu_),2)        #     sigma += sigma_                # sigma = sigma / (num * 3 * 256 * 256)        # sigma = torch.sqrt(sigma)        # # print("check sigma:", sigma.size(), sigma)        # x_s_batch = torch.randn(1000, 3, 256, 256, device=self.device)        # x_s_batch = x_s_batch * sigma + mu        # # flattened = ood_x.view(num, -1).cpu().numpy()        # # mean = np.mean(flattened, axis=0)        # # std = np.std(flattened, axis=0)        # # # print("check mean and std:", mu.cpu().numpy() - mean, sigma.cpu().numpy() - std)        # # def normalize(x):        # #     return (x-mean)/std        # # normalized = normalize(flattened)        # # result = kstest(normalized, norm.cdf, args = (0,1))        # # print(result.statistic)        # # print(result.pvalue)        # # exit()        # # ######   KS test        # # sample_size = (3, 256, 256)        # # batch_size = num        # # # gaussian_samples = torch.normal(mu, sigma, size=(batch_size,)+sample_size)        # # gaussian_samples_2d = x_s_batch.reshape(batch_size, -1)        # # print("gaussian_samples_2d check:", gaussian_samples_2d.size())        # # # gaussian_samples_2d        # # ood_x_2d = ood_x.reshape(batch_size, -1)        # # # print(torch.norm(x-gaussian_samples_2d.mean(dim=0)))        # # D, p = kstest(ood_x_2d.cpu().numpy(), lambda x: np.linalg.norm(x-gaussian_samples_2d.mean(dim=0).cpu().numpy()))        # # print("p-value:", p)        # # exit()        # ref_num = 2        # dis_tol = 50        # ang_tol = 1.0        # ood_1 = ood_x[1,:,:,:].unsqueeze(0)        # ood_2 = ood_x[2,:,:,:].unsqueeze(0)        # for i in range(1000):        #     x_test = x_s_batch[i,:,:,:].unsqueeze(0)        #     ###### rejection         #     dis1 = euclidean_distance(x_test.reshape(-1), ood_1.reshape(-1))        #     dis2 = euclidean_distance(x_test.reshape(-1), ood_2.reshape(-1))        #     ang1 = find_angle(x_test, ood_1, ood_2)        #     ang2 = find_angle(ood_1, ood_2, x_test)        #     ang3 = find_angle(ood_2, ood_1, x_test)        #     ang_o1 = find_angle(origin, x_test, ood_1)        #     ang_o2 = find_angle(origin, x_test, ood_2)        #     ang_ref = find_angle(origin, ood_1, ood_2)        #     print("check dis:", dis1, dis2)        #     print("check ang:", ang1, ang2, ang3)        #     print("check ang with origin:", ang_o1, ang_o2, ang_ref)        #     exit()        #     dis_check = rej_check(577, 5, [dis1, dis2])        #     ang_check1 = rej_check(60, 2, [ang1, ang2, ang3])        #     ang_check2 = rej_check(90, 2, [ang_o1, ang_o2, ang_ref])        #     if dis_check == ang_check1 == ang_check2 == True:        #         print("Pass check:", i)        #         exit()        # exit()        # with torch.no_grad():        #     with tqdm(total=len(seq_inv), desc="Generative process") as progress_bar:        #         for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #             t = (torch.ones(n) * i).to(self.device)        #             t_next = (torch.ones(n) * j).to(self.device)        #             x_test, _ = denoising_step(x_test, t=t, t_next=t_next, models=model,        #                                logvars=self.logvar,        #                                sampling_type=self.args.sample_type,        #                                b=self.betas,        #                                eta=self.args.eta,        #                                learn_sigma=learn_sigma,        #                                ratio=self.args.model_ratio,        #                                hybrid=self.args.hybrid_noise,        #                                hybrid_config=HYBRID_CONFIG,        #                                edit_h=None,        #                                )        #             progress_bar.update(1)        #     x0 = x_test.clone()        #     # save_edit = "id_synthesis_nav1.png"        #     # img_name = str(c)+'_recons.png'        #     img_name = "test_celeba2.png"        #     # tvu.save_image((x0 + 1) * 0.5, os.path.join("/n/fs/yz-diff/BoundaryDiffusion/reconstruction_test/recons_ddpm_celeb_unseen_church/recons/",img_name_recons))        #     tvu.save_image((x0 + 1) * 0.5, os.path.join("/n/fs/yz-diff/BoundaryDiffusion/0227_test/",img_name))                                # exit()        # ###### 0226 TEST Morphological test        # ood_x = torch.load("./ood_samples/iddpm_dog_unseen_church_t500_x.pth").to(self.device)        # ood_h = torch.load("./ood_samples/iddpm_dog_unseen_church_t500_h.pth").to(self.device)        # # ood_x = torch.load("./ood_samples/ddpm_celeba_unseen_dog_t500_x.pth").to(self.device)        # # ood_h = torch.load("./ood_samples/ddpm_celeba_unseen_dog_t500_h.pth").to(self.device)        # # local_center_x_id =  torch.mean(id_x, dim=0).unsqueeze(0)        # # local_center_x_ood =  torch.mean(ood_x, dim=0).unsqueeze(0)        # # print("check euclidean_distance:", euclidean_distance(local_center_x_id.reshape(-1), local_center_x_ood.reshape(-1)))        # # exit()        # num = ood_x.size()[0]        # print("check total num:", num)        # local_center_x = torch.mean(ood_x, dim=0).unsqueeze(0)        # local_center_h = torch.mean(ood_h, dim=0).unsqueeze(0)        # origin = torch.zeros(1, 3, 256, 256, device=self.device)        # x_dis_list = []        # h_dis_list = []        # x_angle_list = []        # h_angle_list = []        # count = 0        # for i in range(1000):        #     p = random.randint(0,num-1)        #     q = random.randint(0,num-1)        #     a = random.randint(0,num-1)        #     if p != q and p != a and q != a:        #     # if p != q:        #         print("random ood sample idx:", count, p, q)        #         count += 1        #         ood_p_x = ood_x[p,:,:,:]        #         ood_q_x = ood_x[q,:,:,:]        #         ood_a_x = ood_x[a,:,:,:]        #         ood_p_h = ood_h[p,:,:,:]        #         ood_q_h = ood_h[q,:,:,:]        #         ood_a_h = ood_h[a,:,:,:]        #         x_dis = euclidean_distance(origin.reshape(-1), ood_q_x.reshape(-1))        #         x_angle = find_angle(origin, ood_p_x, ood_q_x)        #         h_dis = euclidean_distance(ood_p_h.reshape(-1), ood_q_h.reshape(-1))        #         h_angle = find_angle(ood_a_h, ood_p_h, ood_q_h)        #         x_dis_list.append(x_dis)        #         x_angle_list.append(x_angle)        #         h_dis_list.append(h_dis)        #         h_angle_list.append(h_angle)        #         # print("check:", x_dis, x_angle, h_dis, h_angle)        #         # exit()        # ### for x space        # x_dis_list = torch.stack(x_dis_list)        # x_angle_list = torch.stack(x_angle_list)        # print("check mean and std of x_dis:", torch.mean(x_dis_list), torch.std(x_dis_list))        # print("check mean and std of x_angle:", torch.mean(x_angle_list), torch.std(x_angle_list))        ### for h space        # h_dis_list = torch.stack(h_dis_list)        # h_angle_list = torch.stack(h_angle_list)        # print("check mean and std of h_dis:", torch.mean(h_dis_list), torch.std(h_dis_list))        # print("check mean and std of h_angle:", torch.mean(h_angle_list), torch.std(h_angle_list))             # seq_inv = np.linspace(0, 1, 999) * 999        # seq_inv = [int(s) for s in list(seq_inv)]        # seq_inv_next = [-1] + list(seq_inv[:-1])        # id_x = torch.empty(100, 3,256,256)        # id_h = torch.empty(100, 512, 8, 8)         # with torch.no_grad():        #     # get an ID sample with stochastic process        #     x_s_batch = torch.randn(100, 3, 256, 256, device=self.device)        #     for k in range(100):        #         x_s = x_s_batch[k,:,:,:].unsqueeze(0)        #         with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:        #             for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 # print("check step at generation:", t, t_next)        #                 if t == self.args.t_0:        #                     break        #                 x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    # sampling_type='ddim',        #                                    b=self.betas,        #                                    eta=1.0,        #                                    learn_sigma=learn_sigma,        #                                    )        #                 progress_bar.update(1)        #         print("Finish getting an ID latent encoding from sampling with p_s!", x_s.size(), k)        #         id_x[k, :, :, :] = x_s        #         id_h[k, :, :, :] = h_s        # torch.save(id_x, "ddpm_celeb_id_t500_x.pth")        # torch.save(id_h, "ddpm_celeb_id_t500_h.pth")                # exit()                 #######         # ood_x = torch.load("./ood_samples/ddpm_celeba_unseen_church_t500_x.pth").to(self.device)        # ood_h = torch.load("./ood_samples/ddpm_celeba_unseen_church_t500_h.pth").to(self.device)        # num = ood_x.size()[0]        # print("check total num:", num)        # local_center_x = torch.mean(ood_x, dim=0).unsqueeze(0)        # local_center_h = torch.mean(ood_h, dim=0).unsqueeze(0)        # origin = torch.zeros(1, 3, 256, 256, device=self.device)        # x_dis_list = []        # h_dis_list = []        # x_angle_list = []        # h_angle_list = []        # count = 0        # for i in range(num):        #     ood_x_i = ood_x[i,:,:,:]        #     ood_h_i = ood_h[i,:,:,:]        #     x_dis = euclidean_distance(local_center_x.reshape(-1), ood_x_i.reshape(-1))        #     h_dis = euclidean_distance(local_center_h.reshape(-1), ood_h_i.reshape(-1))        #     x_dis_list.append(x_dis)        #     h_dis_list.append(h_dis)        #     # x_angle = find_angle(x_s, ood_p_x, ood_q_x)        #         # x_dis = euclidean_distance(x_s.reshape(-1), ood_q_x.reshape(-1))        #         # x_angle = find_angle(x_s, ood_p_x, ood_q_x)        #         # h_dis = euclidean_distance(h_s.reshape(-1), ood_q_h.reshape(-1))        #         # h_angle = find_angle(h_s, ood_p_h, ood_q_h)        #         # x_dis_list.append(x_dis)        #         # x_angle_list.append(x_angle)        #         # h_dis_list.append(h_dis)        #         # h_angle_list.append(h_angle)        #         # print("check:", x_dis, x_angle, h_dis, h_angle)        #         # exit()        # ### for x space        # x_dis_list = torch.stack(x_dis_list)        # # x_angle_list = torch.stack(x_angle_list)        # print("check mean and std of x_dis:", torch.mean(x_dis_list), torch.std(x_dis_list))        # # print("check mean and std of x_angle:", torch.mean(x_angle_list), torch.std(x_angle_list))        # ### for h space        # h_dis_list = torch.stack(h_dis_list)        # # h_angle_list = torch.stack(h_angle_list)        # print("check mean and std of h_dis:", torch.mean(h_dis_list), torch.std(h_dis_list))        # # print("check mean and std of h_angle:", torch.mean(h_angle_list), torch.std(h_angle_list))        # print("check distance between local center and origin:", euclidean_distance(local_center_x.reshape(-1), origin.reshape(-1)))        # exit()        #### ID-OOD angle map        ### o2m mapping        ####### One to Many Mapping        # ### prepare a group of OOD images, invert them to the same latent spaces as x_s and h_s        # with torch.no_grad():        #     # get an ID sample with stochastic process        #     seq_inv = np.linspace(0, 1, 999) * 999        #     seq_inv = [int(s) for s in list(seq_inv)]        #     seq_inv_next = [-1] + list(seq_inv[:-1])        #     x_s = torch.randn(1, 3, 256, 256, device=self.device)        #     with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:        #         for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #             t = (torch.ones(n) * i).to(self.device)        #             t_next = (torch.ones(n) * j).to(self.device)        #             # print("check step at generation:", t, t_next)        #             if t == self.args.t_0:        #                 break        #             x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                logvars=self.logvar,        #                                sampling_type=self.args.sample_type,        #                                # sampling_type='ddim',        #                                b=self.betas,        #                                eta=1.0,        #                                learn_sigma=learn_sigma,        #                                )        #             progress_bar.update(1)        #     print("Finish getting an ID latent encoding from sampling with p_s!", x_s.size())        # x_dis_list = []        # h_dis_list = []        # x_angle_list = []        # h_angle_list = []        # count = 0        # for i in range(1000):        #     p = random.randint(0,num-1)        #     q = random.randint(0,num-1)        #     # a = random.randint(0,num-1)        #     # if p != q and p != a and q != a:        #     if p != q:        #         print("random ood sample idx:", count, p, q)        #         count += 1        #         ood_p_x = ood_x[p,:,:,:]        #         ood_q_x = ood_x[q,:,:,:]        #         # ood_a_x = ood_x[a,:,:,:]        #         ood_p_h = ood_h[p,:,:,:]        #         ood_q_h = ood_h[q,:,:,:]        #         # ood_a_h = ood_h[a,:,:,:]        #         x_dis = euclidean_distance(x_s.reshape(-1), ood_q_x.reshape(-1))        #         x_angle = find_angle(x_s, ood_p_x, ood_q_x)        #         h_dis = euclidean_distance(h_s.reshape(-1), ood_q_h.reshape(-1))        #         h_angle = find_angle(h_s, ood_p_h, ood_q_h)        #         x_dis_list.append(x_dis)        #         x_angle_list.append(x_angle)        #         h_dis_list.append(h_dis)        #         h_angle_list.append(h_angle)        #         # print("check:", x_dis, x_angle, h_dis, h_angle)        #         # exit()        # ### for x space        # x_dis_list = torch.stack(x_dis_list)        # x_angle_list = torch.stack(x_angle_list)        # print("check mean and std of x_dis:", torch.mean(x_dis_list), torch.std(x_dis_list))        # print("check mean and std of x_angle:", torch.mean(x_angle_list), torch.std(x_angle_list))        # ### for h space        # h_dis_list = torch.stack(h_dis_list)        # h_angle_list = torch.stack(h_angle_list)        # print("check mean and std of h_dis:", torch.mean(h_dis_list), torch.std(h_dis_list))        # print("check mean and std of h_angle:", torch.mean(h_angle_list), torch.std(h_angle_list))        # x_dis_list = []        # h_dis_list = []        # x_angle_list = []        # h_angle_list = []        # count = 0        # for i in range(1000):        #     p = random.randint(0,num-1)        #     q = random.randint(0,num-1)        #     a = random.randint(0,num-1)        #     if p != q and p != a and q != a:        #         print("random ood sample idx:", count, p, q, a)        #         count += 1        #         ood_p_x = ood_x[p,:,:,:]        #         ood_q_x = ood_x[q,:,:,:]        #         ood_a_x = ood_x[a,:,:,:]        #         ood_p_h = ood_h[p,:,:,:]        #         ood_q_h = ood_h[q,:,:,:]        #         ood_a_h = ood_h[a,:,:,:]        #         x_dis = euclidean_distance(ood_p_x.reshape(-1), ood_q_x.reshape(-1))        #         x_angle = find_angle(ood_a_x, ood_p_x, ood_q_x)        #         h_dis = euclidean_distance(ood_p_h.reshape(-1), ood_q_h.reshape(-1))        #         h_angle = find_angle(ood_a_h, ood_p_h, ood_q_h)        #         x_dis_list.append(x_dis)        #         x_angle_list.append(x_angle)        #         h_dis_list.append(h_dis)        #         h_angle_list.append(h_angle)        #         # print("check:", x_dis, x_angle, h_dis, h_angle)        #         # exit()        # ### for x space        # x_dis_list = torch.stack(x_dis_list)        # x_angle_list = torch.stack(x_angle_list)        # print("check mean and std of x_dis:", torch.mean(x_dis_list), torch.std(x_dis_list))        # print("check mean and std of x_angle:", torch.mean(x_angle_list), torch.std(x_angle_list))        # ### for h space        # h_dis_list = torch.stack(h_dis_list)        # h_angle_list = torch.stack(h_angle_list)        # print("check mean and std of h_dis:", torch.mean(h_dis_list), torch.std(h_dis_list))        # print("check mean and std of h_angle:", torch.mean(h_angle_list), torch.std(h_angle_list))        # for i in range(100):        #     nums = random.sample(range(0, 100), 3)        #     print("check nums:", nums)        #     xo_A = ood_x[nums[0],:,:,:].unsqueeze(0)        #     xo_B = ood_x[nums[1],:,:,:].unsqueeze(0)        #     xo_C = ood_x[nums[2],:,:,:].unsqueeze(0)        #     ### Find the mean direction defined by AB and AC        #     AB = (xo_B - xo_A) / torch.norm(xo_B - xo_A)        #     AC = (xo_C - xo_A) / torch.norm(xo_C - xo_A)        #     dot_p = torch.dot(AB.view(-1), AC.view(-1))        #     AB_magnitude = torch.norm(AB)        #     AC_magnitude = torch.norm(AC)        #     angle = torch.acos( dot_p / (AB_magnitude * AC_magnitude))        #     angle_degrees = angle * 180 / torch.tensor(3.14159)        #     print("check dot_p:", torch.sum(dot_p), angle_degrees)        #     angle_list.append(angle_degrees)        #     # exit()        # angle_list = torch.stack(angle_list)        # print("check mean and std of angles:", torch.mean(angle_list), torch.std(angle_list))        ##        # ood_x = torch.load("./0220_test/subspace/ood_x.pth").to(self.device)        # recons_mse = 0        # recons_mae = 0        # imgs = glob("/n/fs/yz-diff/BoundaryDiffusion/syn_imgs/train/dog/*")        # random.shuffle(imgs)        # ood_x = torch.empty((len(imgs),3,256,256))        # ood_h = torch.empty(len(imgs), 512, 8, 8)        # for c, im in enumerate(imgs):        #     img = Image.open(im).convert("RGB")        #     img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        #     img = np.array(img)/255        #     img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        #     img = img.to(self.config.device)        #     img_name = str(c)+'_orig.png'        #     tvu.save_image(img, os.path.join("/n/fs/yz-diff/BoundaryDiffusion/reconstruction_test/recons_iddpm_dog/img_samples/", img_name))        #     x0 = (img - 0.5) * 2.        #     with torch.no_grad():        #         #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#        #         if self.args.deterministic_inv:        #             # x_lat_path = os.path.join(self.args.image_folder, f'x_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}.pth')        #             # h_lat_path = os.path.join(self.args.image_folder, f'h_lat_t{self.args.t_0}_ninv{self.args.n_inv_step}.pth')        #             # if not os.path.exists(x_lat_path):        #             seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        #             seq_inv = [int(s) for s in list(seq_inv)]        #             seq_inv_next = [-1] + list(seq_inv[:-1])        #             x = x0.clone()        #             with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:        #                 for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):        #                     t = (torch.ones(n) * i).to(self.device)        #                     t_prev = (torch.ones(n) * j).to(self.device)        #                     x, mid_h_g = denoising_step(x, t=t, t_next=t_prev, models=model,        #                                        logvars=self.logvar,        #                                        sampling_type='ddim',        #                                        b=self.betas,        #                                        eta=0,        #                                        learn_sigma=learn_sigma,        #                                        ratio=0,        #                                        )        #                     progress_bar.update(1)        #                 x_lat = x.clone()        #                 h_lat = mid_h_g.clone()        #     print("Finish inversion for the given image to step:", self.args.t_0, x_lat.size(), h_lat.size(), c)                    #     # ood_x[c, :, :, :] = x_lat        #     # ood_h[c, :, :, :] = h_lat                   #     x_s = x_lat        #     with torch.no_grad():        #         with tqdm(total=len(seq_inv), desc="Generative process") as progress_bar:        #             for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 # print("check eta:", self.args.eta)        #                 x_s, _ = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    # eta=self.args.eta,        #                                    eta = 0.0,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    edit_h=None,        #                                    )        #                 progress_bar.update(1)        #         # x0 = x.clone()        #         img_name_recons = str(c)+'_recons.png'        #         tvu.save_image((x_s + 1) * 0.5, os.path.join("/n/fs/yz-diff/BoundaryDiffusion/reconstruction_test/recons_iddpm_dog/recons/",img_name_recons))        #     mse_i = F.mse_loss(img, (x_s + 1) * 0.5).item()        #     mae_i = F.l1_loss(img, (x_s + 1) * 0.5).item()        #     print("check recons losses:", mse_i, mae_i)        #     recons_mse += mse_i        #     recons_mae += mae_i        # # torch.save(ood_x, "ddpm_celeba_unseen_church_t500_x.pth")        # # torch.save(ood_h, "ddpm_celeba_unseen_church_t500_h.pth")        # print("Final loss check:", recons_mse/len(imgs), recons_mae/len(imgs))        ###### 0225 test         # train_dataset, test_dataset = get_dataset(self.config.data.dataset, DATASET_PATHS, self.config)        # loader_dic = get_dataloader(train_dataset, test_dataset, bs_train=self.args.bs_train,        #                             num_workers=self.config.data.num_workers)        # loader = loader_dic['train']        # for step, img in enumerate(loader):        #     if step < 100:        #         x0 = img.to(self.config.device)        #         print("check dataloader for LSUN dataset!")        #         img_path = str(step)+".png"        #         tvu.save_image((x0 + 1) * 0.5, os.path.join("/n/fs/yz-diff/BoundaryDiffusion/syn_imgs/train/church/", img_path))        # ood_x = torch.load("./0220_test/subspace/ood_x.pth").to(self.device)        # #### get the initial h_s and x_s        # seq_inv = np.linspace(0, 1, self.args.n_inv_step) * self.args.t_0        # seq_inv = [int(s) for s in list(seq_inv)]        # seq_inv_next = [-1] + list(seq_inv[:-1])         # x_s_batch = torch.randn(100, 3, 256, 256, device=self.device)        # xo_1 = ood_x[0,:,:,:].unsqueeze(0)        # xo_2 = ood_x[1,:,:,:].unsqueeze(0)        # xo_3 = ood_x[2,:,:,:].unsqueeze(0)        # # r = random.randint(0,100)        # # x_s = ood_x[r,:,:,:].unsqueeze(0)        # # diff_dis = euclidean_distance(xo_1, x_s)        # # print("check distance and angle:", diff_dis)        # time_in_start = time.time()        # normal_vectors = generate_directions(xo_1.view(-1).cpu(), xo_2.view(-1).cpu(), 3*256*256, 100)        # time_in_end = time.time()        # print("check normal_vectors:", time_in_end - time_in_start, normal_vectors.size())        # torch.save(normal_vectors,"normal_vectors.pth")        # # exit()        # for k in range(100):        #     # with torch.no_grad():        #     #     # x_s = torch.randn(1, 3, 256, 256, device=self.device)        #     #     x_s = x_s_batch[k,:,:,:].unsqueeze(0).to(self.device)        #     #     with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:        #     #         for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #     #             t = (torch.ones(n) * i).to(self.device)        #     #             t_next = (torch.ones(n) * j).to(self.device)        #     #             x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,        #     #                                logvars=self.logvar,        #     #                                sampling_type=self.args.sample_type,        #     #                                # sampling_type='ddim',        #     #                                b=self.betas,        #     #                                eta=1.0,        #     #                                learn_sigma=learn_sigma,        #     #                                )        #     #             progress_bar.update(1)        #     #     print("Finish getting an ID latent encoding from sampling with p_s at the mixing step!")              #     ### get a random direction norm vector        #     # xo_1 = ood_x[0,:,:,:].unsqueeze(0)        #     # xo_2 = ood_x[1,:,:,:].unsqueeze(0)        #     # xo_3 = ood_x[2,:,:,:].unsqueeze(0)        #     # # r = random.randint(0,100)        #     # # x_s = ood_x[r,:,:,:].unsqueeze(0)        #     # # diff_dis = euclidean_distance(xo_1, x_s)        #     # # print("check distance and angle:", diff_dis)        #     # normal_vectors = generate_directions(xo_1.view(-1).cpu(), xo_2.view(-1).cpu(), 3*256*256, 100)        #     # print("check normal_vectors:", normal_vectors.size())        #     # exit()        #     # normal = (x_s - xo_1) / torch.norm(x_s - xo_1)        #     normal = normal_vectors[k,:].view(-1, 3, 256, 256).to(self.device)        #     xo_c_pos = xo_1 + 580 * normal         #     xo_c_neg = xo_1 - 580 * normal        #     # xo_c        #     with torch.no_grad():        #         with tqdm(total=len(seq_inv), desc="Generative process") as progress_bar:        #             # edit_z = editing_seq[q]        #             # edit_z = xo_c.view(-1, 3, 256, 256)        #             edit_z = xo_c_pos        #             for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 edit_z, _ = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    eta = self.args.eta,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    # edit_h=edit_h,        #                                    )        #         # x0 = x.clone()        #         save_edit = "navigation_pos_"+str(k)+".png"        #         tvu.save_image((edit_z + 1) * 0.5, os.path.join("0222_test/navigation_test4/",save_edit))        #         time_in_end = time.time()        #         with tqdm(total=len(seq_inv), desc="Generative process") as progress_bar:        #             edit_z = xo_c_neg        #             for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 edit_z, _ = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    eta = self.args.eta,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    # edit_h=edit_h,        #                                    )        #         # x0 = x.clone()        #         save_edit = "navigation_neg_"+str(k)+".png"        #         tvu.save_image((edit_z + 1) * 0.5, os.path.join("0222_test/navigation_test4/",save_edit))        #         time_in_end = time.time()                                         # for k in range(100):        #     # with torch.no_grad():        #     #     # x_s = torch.randn(1, 3, 256, 256, device=self.device)        #     #     x_s = x_s_batch[k,:,:,:].unsqueeze(0).to(self.device)        #     #     with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:        #     #         for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #     #             t = (torch.ones(n) * i).to(self.device)        #     #             t_next = (torch.ones(n) * j).to(self.device)        #     #             x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,        #     #                                logvars=self.logvar,        #     #                                sampling_type=self.args.sample_type,        #     #                                # sampling_type='ddim',        #     #                                b=self.betas,        #     #                                eta=1.0,        #     #                                learn_sigma=learn_sigma,        #     #                                )        #     #             progress_bar.update(1)        #     #     print("Finish getting an ID latent encoding from sampling with p_s at the mixing step!")        #     ####### Test 0222 - new trajectory navigation        #     # ood_x = torch.load("./0220_test/subspace/ood_x.pth").to(self.device)                #     step_length = 40        #     max_steps = 5        #     angle_list = []        #     xo_1 = ood_x[0,:,:,:].unsqueeze(0)        #     xo_2 = ood_x[1,:,:,:].unsqueeze(0)        #     xo_3 = ood_x[2,:,:,:].unsqueeze(0)        #     # xo_1.requires_grad=True        #     # xo_1.requires_grad=True        #     # direction_norm = (xo_2 - xo_1) / torch.norm(xo_2 - xo_1)        #     target_angle = torch.deg2rad(torch.tensor(60))        #     n_dir = x_s_batch[k,:,:,:].unsqueeze(0).to(self.device)        #     normal = (xo_1 - n_dir) / torch.norm(xo_1 - n_dir)        #     # normal = (xo_3 - xo_1) / torch.norm(xo_3 - xo_1)        #     # print()        #     # exit()        #     # normal = x_s_batch[k,:,:,:].unsqueeze(0).to(self.device)        #     time_in_start = time.time()        #     xo_c = find_equilateral_point(xo_1.view(-1), xo_2.view(-1), normal.view(-1), torch.tensor(60))        #     time_in_end = time.time()        #     print("Find a potential new sample in:", time_in_end - time_in_start, xo_c.size())        #     editing_seq = []        #     xo_c = xo_c.view(-1,3,256,256)        #     for i in range(max_steps):        #         x_edit = xo_c + i * step_length * normal        #         # print("check x_edit:", i, x_edit.size())        #         editing_seq.append(x_edit)        #     with torch.no_grad():        #         for q in range(len(editing_seq)):        #             with tqdm(total=len(seq_inv), desc="Generative process") as progress_bar:        #                 edit_z = editing_seq[q]        #                 # edit_z = xo_c.view(-1, 3, 256, 256)        #                 for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                     t = (torch.ones(n) * i).to(self.device)        #                     t_next = (torch.ones(n) * j).to(self.device)        #                     edit_z, _ = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                        logvars=self.logvar,        #                                        sampling_type=self.args.sample_type,        #                                        b=self.betas,        #                                        eta = self.args.eta,        #                                        learn_sigma=learn_sigma,        #                                        ratio=self.args.model_ratio,        #                                        hybrid=self.args.hybrid_noise,        #                                        hybrid_config=HYBRID_CONFIG,        #                                        # edit_h=edit_h,        #                                        )        #             # x0 = x.clone()        #             save_edit = "navigation_pos_"+str(k)+"_"+str(q)+".png"        #             tvu.save_image((edit_z + 1) * 0.5, os.path.join("0222_test/navigation_test3/",save_edit))        #             time_in_end = time.time()        #     # print(f"Generating for 1 image takes {time_in_end - time_in_start:.4f}s")         #     # exit()            #### Test 0221 - with the trajectory width to be 120, OOD sample distance at step t=500 ~ 577        ################ ID sample distance at step t=T ~ 628, ID sample distance at step t=500 ~ 50        ################ should be separable enough to goes into the OOD unseen subspace ?         ############# Test 1. Navigation, direction towards a known OOD sample        # ood_x = torch.load("./0220_test/subspace/ood_x.pth").to(self.device)        # step_length = 40        # max_steps = 30        # ## test w/ idx 0 and idx 1        # xo_1 = ood_x[0,:,:,:].unsqueeze(0)        # xo_2 = ood_x[1,:,:,:].unsqueeze(0)        # direction_norm = (xo_2 - xo_1) / torch.norm(xo_2 - xo_1)        # # editing_seq = np.linspace(0, 600, max_steps)        # editing_seq = []        # for i in range(max_steps):        #     x_edit = xo_1 + i * step_length * direction_norm        #     # print("check x_edit:", i, x_edit.size())        #     editing_seq.append(x_edit)        # with torch.no_grad():        #     for k in range(len(editing_seq)):        #         with tqdm(total=len(seq_inv), desc="Generative process {}".format(k)) as progress_bar:        #             edit_z = editing_seq[k]        #             for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 edit_z, _ = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    eta = self.args.eta,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    # edit_h=edit_h,        #                                    )        #         # x0 = x.clone()        #         save_edit = "navigation1_"+str(k)+".png"        #         tvu.save_image((edit_z + 1) * 0.5, os.path.join("0221_test/navigation_test1/",save_edit))                # time_in_end = time.time()                # print(f"Generating for 1 image takes {time_in_end - time_in_start:.4f}s")             # exit()            ##### this test meets my expectation           ############# Test 2. navigation, random direction defined by an ID sample towards a OOD sample        # ood_x = torch.load("./0220_test/subspace/ood_x.pth").to(self.device)        # step_length = 40        # max_steps = 30        # xo = ood_x[0,:,:,:].unsqueeze(0)        # with torch.no_grad():        #     x_s = torch.randn(1, 3, 256, 256, device=self.device)        #     with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:        #         for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #             t = (torch.ones(n) * i).to(self.device)        #             t_next = (torch.ones(n) * j).to(self.device)        #             x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                logvars=self.logvar,        #                                sampling_type=self.args.sample_type,        #                                # sampling_type='ddim',        #                                b=self.betas,        #                                eta=1.0,        #                                learn_sigma=learn_sigma,        #                                )        #             progress_bar.update(1)        #     print("Finish getting an ID latent encoding from sampling with p_s at the mixing step!")        # #### direction towards xo        # direction_norm = (x_s - xo) / torch.norm(x_s - xo)        # # editing_seq = np.linspace(0, 600, max_steps)        # editing_seq = []        # for i in range(max_steps):        #     x_edit = xo - i * step_length * direction_norm        #     # print("check x_edit:", i, x_edit.size())        #     editing_seq.append(x_edit)         # with torch.no_grad():        #     for k in range(len(editing_seq)):        #         with tqdm(total=len(seq_inv), desc="Generative process {}".format(k)) as progress_bar:        #             edit_z = editing_seq[k]        #             for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 edit_z, _ = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    eta = self.args.eta,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    # edit_h=edit_h,        #                                    )        #         # x0 = x.clone()        #         save_edit = "navigation_neg_"+str(k)+".png"        #         tvu.save_image((edit_z + 1) * 0.5, os.path.join("0221_test/navigation_test2/",save_edit))            # Also as expected, a random direction of course does not garantee the right optimization direction to go        ############# Test 3. navigation, direction towards the intersection of two OOD samples         # ood_x = torch.load("./0220_test/subspace/ood_x.pth").to(self.device)        # ood_x = torch.randn(100, 3, 256, 256, device=self.device)        # step_length = 40        # max_steps = 30        # angle_list = []        # for i in range(100):        #     nums = random.sample(range(0, 100), 3)        #     print("check nums:", nums)        #     xo_A = ood_x[nums[0],:,:,:].unsqueeze(0)        #     xo_B = ood_x[nums[1],:,:,:].unsqueeze(0)        #     xo_C = ood_x[nums[2],:,:,:].unsqueeze(0)        #     ### Find the mean direction defined by AB and AC        #     AB = (xo_B - xo_A) / torch.norm(xo_B - xo_A)        #     AC = (xo_C - xo_A) / torch.norm(xo_C - xo_A)        #     dot_p = torch.dot(AB.view(-1), AC.view(-1))        #     AB_magnitude = torch.norm(AB)        #     AC_magnitude = torch.norm(AC)        #     angle = torch.acos( dot_p / (AB_magnitude * AC_magnitude))        #     angle_degrees = angle * 180 / torch.tensor(3.14159)        #     print("check dot_p:", torch.sum(dot_p), angle_degrees)        #     angle_list.append(angle_degrees)        #     # exit()        # angle_list = torch.stack(angle_list)        # print("check mean and std of angles:", torch.mean(angle_list), torch.std(angle_list))        # exit()        # AD = torch.cross(AB, AC)        # # print("check AD:", AD.size())        # direction_norm = AD / torch.norm(AD)        # # exit()        # editing_seq = []        # for i in range(max_steps):        #     x_edit = xo_A - i * step_length * direction_norm        #     editing_seq.append(x_edit)                 # with torch.no_grad():        #     for k in range(10):        #         with tqdm(total=len(seq_inv), desc="Generative process {}".format(k)) as progress_bar:        #             # edit_z = editing_seq[k]        #             edit_z = ood_x[k,:,:,:].unsqueeze(0)        #             for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 edit_z, _ = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    eta = self.args.eta,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    # edit_h=edit_h,        #                                    )        #         # x0 = x.clone()        #         save_edit = "navigation_"+str(k)+".png"        #         tvu.save_image((edit_z + 1) * 0.5, os.path.join("0221_test/geometric_test/",save_edit))        ######### Test 4 Subspace capturing, a spherical shell of radius 570        # samples = sample_spherical_shell(580, 3*256*256, 10)        # samples = torch.from_numpy(samples).to(self.device).to(torch.float)        # samples = samples.view(-1, 3, 256, 256)        # print("check sample size:", samples.size())        # with torch.no_grad():        #     for k in range(10):        #         with tqdm(total=len(seq_inv), desc="Generative process {}".format(k)) as progress_bar:        #             # edit_z = editing_seq[k]        #             edit_z = samples[k,:,:,:].unsqueeze(0)        #             for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 edit_z, _ = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    eta = self.args.eta,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    # edit_h=edit_h,        #                                    )        #         # x0 = x.clone()        #         save_edit = "navigation_"+str(k)+".png"        #         tvu.save_image((edit_z + 1) * 0.5, os.path.join("0221_test/geometric_test/",save_edit))        # ood_x = torch.load("./0220_test/subspace/ood_x.pth").to(self.device)        # x_s_batch = torch.randn(100, 3, 256, 256, device=self.device)        # dis = 0        # with torch.no_grad():        #     for k in range(100):        #         x_s = x_s_batch[k,:,:,:].unsqueeze(0)        #         with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:        #             for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    # sampling_type='ddim',        #                                    b=self.betas,        #                                    eta=1.0,        #                                    learn_sigma=learn_sigma,        #                                    )        #                 progress_bar.update(1)        #         print("Finish getting an ID latent encoding from sampling with p_s at the mixing step!")        #         # ood_x[k,:,:,:] = x_s          #         dis += euclidean_distance(ood_x[k, :, :, :].unsqueeze(0), x_s)        #         print("distance between ID and OOD:", euclidean_distance(ood_x[k, :, :, :].unsqueeze(0), x_s))        # print("check avg distance:", dis/100)        #### Test 0220 - unseen subspace capturing test 1: use two OOD samples as defining point        #### pre-compute the ood samples        #### hyper-param notes: --n_inv_step 60 --t_0 500         # imgs = glob("/n/fs/yz-diff/BoundaryDiffusion/syn_imgs/train/human/pos/*")        # # random.shuffle(imgs)        # imgs = sorted(imgs)        # ood_x = torch.empty((len(imgs),3,256,256))        # ood_h = torch.empty(len(imgs), 512, 8, 8)        # for c, im in enumerate(imgs):        #     img = Image.open(im).convert("RGB")        #     img = img.resize((self.config.data.image_size, self.config.data.image_size), Image.ANTIALIAS)        #     img = np.array(img)/255        #     img = torch.from_numpy(img).type(torch.FloatTensor).permute(2, 0, 1).unsqueeze(dim=0).repeat(n, 1, 1, 1)        #     img = img.to(self.config.device)        #     # img_name = str(c)+'_orig.png'        #     # tvu.save_image(img, "ood_origin.png")        #     x0 = (img - 0.5) * 2.        #     with torch.no_grad():        #         #---------------- Invert Image to Latent in case of Deterministic Inversion process -------------------#        #         if self.args.deterministic_inv:        #             x = x0.clone()        #             with tqdm(total=len(seq_inv), desc=f"Inversion process ") as progress_bar:        #                 for it, (i, j) in enumerate(zip((seq_inv_next[1:]), (seq_inv[1:]))):        #                     t = (torch.ones(n) * i).to(self.device)        #                     t_prev = (torch.ones(n) * j).to(self.device)        #                     x, mid_h_g = denoising_step(x, t=t, t_next=t_prev, models=model,        #                                        logvars=self.logvar,        #                                        sampling_type='ddim',        #                                        b=self.betas,        #                                        eta=0,        #                                        learn_sigma=learn_sigma,        #                                        ratio=0,        #                                        )        #                     progress_bar.update(1)        #                 x_lat = x.clone()        #                 h_lat = mid_h_g.clone()        #                 ood_x[c,:,:,:] = x_lat        #                 ood_h[c,:,:,:] = h_lat        # torch.save(ood_x,"./0220_test/subspace/ood_x.pth")        # torch.save(ood_h, "./0220_test/subspace/ood_h.pth")        # print("Finish saving the ood samples!")        #### load the pre-compute ood samples and define the hyperplanes using the random samples        #### save those hyperplanes to ./0220_test/subspace/random_hyperplanes        # ood_h = torch.load("./0220_test/subspace/ood_h.pth").to(self.device)        # ood_x = torch.load("./0220_test/subspace/ood_x.pth").to(self.device)        # print("check pre-computed latent OOD samples:", ood_x.size(), ood_h.size())        # # try to get 100 hyperplanes        # for n in range(100):        #     i = random.randint(0, 100)        #     j = random.randint(0, 100)        #     if i != j:        #         x_1 = ood_x[i,:,:,:]        #         x_2 = ood_x[j,:,:,:]        #         h_1 = ood_h[i,:,:,:]        #         h_2 = ood_h[j,:,:,:]        #         coef_x1 = point_hyper(x_1.view(-1), x_2.view(-1)).unsqueeze(0).numpy()        #         coef_h1 = point_hyper(h_1.view(-1), h_2.view(-1)).unsqueeze(0).numpy()        #         print(np.shape(coef_x1), np.shape(coef_h1))        #         save_x = './0220_test/subspace/random_hyperplanes/human_coef_x'+ str(n)+'.npy'        #         save_h = './0220_test/subspace/random_hyperplanes/human_coef_h'+ str(n)+'.npy'        #         np.save(save_x, coef_x1)        #         np.save(save_h, coef_h1)        #         print("Finish hyperplane search for OOD img:", n)        ######### Navigation test, goal is to navigate the ID sample to the OOD high concentration area        #### Get a random ID sample first        # ood_x = torch.empty(100,3,256,256)        # x_s_batch = torch.randn(100, 3, 256, 256, device=self.device)        # with torch.no_grad():        #     for k in range(100):        #         x_s = x_s_batch[k,:,:,:].unsqueeze(0)        #         with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:        #             for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    # sampling_type='ddim',        #                                    b=self.betas,        #                                    eta=1.0,        #                                    learn_sigma=learn_sigma,        #                                    )        #                 progress_bar.update(1)        #         print("Finish getting an ID latent encoding from sampling with p_s at the mixing step!")        #         ood_x[k,:,:,:] = x_s        #### subspace sampling        # ood_h = torch.load("./0220_test/subspace/ood_h.pth").to(self.device)        # ood_x = torch.load("./0220_test/subspace/ood_x.pth").to(self.device)        # ### estimate the distance between two latent OOD samples        # total_dis = 0        # # ood_x = torch.randn(100, 3, 256, 256, device=self.device)        # for i in range(100):        #     p = random.randint(0,99)        #     q = random.randint(0,99)        #     xo_1 = ood_x[p,:,:,:].unsqueeze(0)        #     xo_2 = ood_x[q,:,:,:].unsqueeze(0)        #     dis_od = euclidean_distance(xo_1,xo_2)        #     print("check distance:", i, dis_od)        #     total_dis += dis_od        # print("check avg distance:", total_dis/100)        # with torch.no_grad():        #     k = random.randint(0,102)        #     print("get the human image sample:", k)        #     time_in_start = time.time()        #     x_s = ood_x[k,:,:,:].unsqueeze(0)        #     # h = ood_h[k,:,:,:].unsqueeze(0)        #     coef_x1 = np.load('./0220_test/subspace/random_hyperplanes/human_coef_x1.npy')        #     print("dim of coef_x1:", np.shape(coef_x1))        #     # x_s = torch.from_numpy(coef_x1).to(self.device).view(-1, 3, 256, 256)        #     z_linspace = np.linspace(-60, 60, 30)        #     z_latent_code = x_s.cpu().view(1,-1).numpy()        #     z_linspace = z_linspace - z_latent_code.dot(coef_x1.T)        #     z_linspace = z_linspace.reshape(-1, 1).astype(np.float32)        #     edit_z_seq = z_latent_code + z_linspace * coef_x1           #     for k in range(1):        #         with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:        #             # edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)        #             edit_z = torch.from_numpy(edit_z_seq[-1]).to(self.device).view(-1, 3, 256, 256)        #             for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 edit_z, _ = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    eta = self.args.eta,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    # edit_h=edit_h,        #                                    )        #         # x0 = x.clone()        #         save_edit = "navigation1_"+str(k)+".png"        #         tvu.save_image((edit_z + 1) * 0.5, os.path.join("0220_test/subspace/navigation_test",save_edit))        #         time_in_end = time.time()        #         print(f"Generating for 1 image takes {time_in_end - time_in_start:.4f}s")         #         # exit()             #### Test with the vanilla nagivation        # coef_x1 = np.load('./0220_test/subspace/random_hyperplanes/human_coef_x0.npy')        # coef_h1 = np.load('./0220_test/subspace/random_hyperplanes/human_coef_h0.npy')             # edit_h_seq, edit_z_seq, h_s, x_s = navigation_iter(-300, 300,  30, 20, coef_x1, coef_h1, h_s, x_s)         # with torch.no_grad():        #     for k in range(len(edit_z_seq)):        #         time_in_start = time.time()        #         with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:        #             edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)        #             edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)        #             for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                 t = (torch.ones(n) * i).to(self.device)        #                 t_next = (torch.ones(n) * j).to(self.device)        #                 edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                    logvars=self.logvar,        #                                    sampling_type=self.args.sample_type,        #                                    b=self.betas,        #                                    eta = self.args.eta,        #                                    learn_sigma=learn_sigma,        #                                    ratio=self.args.model_ratio,        #                                    hybrid=self.args.hybrid_noise,        #                                    hybrid_config=HYBRID_CONFIG,        #                                    edit_h=edit_h,        #                                    )        #         # x0 = x.clone()        #         save_edit = "random_traj1_nav"+str(k)+".png"        #         tvu.save_image((edit_z + 1) * 0.5, os.path.join("0220_test/subspace/vanilla_navigation",save_edit))        #         time_in_end = time.time()        #         print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")        #     coef_x1 = np.load('./0220_test/subspace/random_hyperplanes/human_coef_x1.npy')        #     coef_h1 = np.load('./0220_test/subspace/random_hyperplanes/human_coef_h1.npy')             #     edit_h_seq, edit_z_seq, h_s, x_s = navigation_iter(-300, 300,  30, 20, coef_x1, coef_h1, h_s, x_s)         #     with torch.no_grad():        #         for k in range(len(edit_z_seq)):        #             time_in_start = time.time()        #             with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:        #                 edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)        #                 edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)        #                 for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #                     t = (torch.ones(n) * i).to(self.device)        #                     t_next = (torch.ones(n) * j).to(self.device)        #                     edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                        logvars=self.logvar,        #                                        sampling_type=self.args.sample_type,        #                                        b=self.betas,        #                                        eta = self.args.eta,        #                                        learn_sigma=learn_sigma,        #                                        ratio=self.args.model_ratio,        #                                        hybrid=self.args.hybrid_noise,        #                                        hybrid_config=HYBRID_CONFIG,        #                                        edit_h=edit_h,        #                                        )        #             # x0 = x.clone()        #             save_edit = "random_traj2_nav"+str(k)+".png"        #             tvu.save_image((edit_z + 1) * 0.5, os.path.join("0220_test/subspace/vanilla_navigation",save_edit))        #             time_in_end = time.time()        #             print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")        # # note this should be differnt from the seed used to get the hyperplanes (1006)        # x_s_batch = torch.randn(100, 3, 256, 256, device=self.device)        # with torch.no_grad():        #     x_s = x_s_batch[1,:,:,:].unsqueeze(0)        #     with tqdm(total=len(seq_inv), desc=f"Generative process") as progress_bar:        #         for it, (i, j) in enumerate(zip(reversed((seq_inv)), reversed((seq_inv_next)))):        #             t = (torch.ones(n) * i).to(self.device)        #             t_next = (torch.ones(n) * j).to(self.device)        #             x_s, h_s = denoising_step(x_s, t=t, t_next=t_next, models=model,        #                                logvars=self.logvar,        #                                sampling_type=self.args.sample_type,        #                                # sampling_type='ddim',        #                                b=self.betas,        #                                eta=1.0,        #                                learn_sigma=learn_sigma,        #                                )        #             progress_bar.update(1)        #     print("Finish getting an ID latent encoding from sampling with p_s at the mixing step!")          ###### Vanilla navigation test 1        # # boundary_idx = torch.randint(0,102,(100,)) ## k        # # print(boundary_idx)        # diff_distance_list = [300,200,200,200,200,200,150,150,150,100,100,100,100,100]        # for c in range(len(diff_distance_list)):        #     j = random.randint(0,102)        #     boundary_z_path = './test2_center/subspace_500_origin/human_coef_x'+str(j)+'.npy'        #     boundary_h_path = './test2_center/subspace_500_origin/human_coef_h'+str(j)+'.npy'        #     coef_x1 = np.load(boundary_z_path)        #     coef_h1 = np.load(boundary_h_path)             #     # if c == 0:        #     #     diff_distance = 300        #     # else:        #     #     diff_distance = 200        #     diff_distance = diff_distance_list[c]        #     k = random.randint(0,49)        #     edit_h_seq, edit_z_seq, h_s, x_s = navigation_iter(-diff_distance, diff_distance, 50, k, coef_x1, coef_h1, h_s, x_s)         #     edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)        #     edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)                        # with torch.no_grad():            #     # for k in range(len(edit_z_seq)):            #     time_in_start = time.time()            #     with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:            #         # edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)            #         # edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)            #         for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):            #             t = (torch.ones(n) * i).to(self.device)            #             t_next = (torch.ones(n) * j).to(self.device)            #             edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,            #                                logvars=self.logvar,            #                                sampling_type=self.args.sample_type,            #                                b=self.betas,            #                                eta = self.args.eta,            #                                learn_sigma=learn_sigma,            #                                ratio=self.args.model_ratio,            #                                hybrid=self.args.hybrid_noise,            #                                hybrid_config=HYBRID_CONFIG,            #                                edit_h=edit_h,            #                                )            #     # x0 = x.clone()            #     save_edit = "unseen_traj_nav"+str(c)+".png"            #     tvu.save_image((edit_z + 1) * 0.5, os.path.join("navigation_result",save_edit))            #     time_in_end = time.time()            #     print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")        ##### Projection test 2        # normals_z = []        # normals_h = []        # local_center_x = torch.load("./test2_center/human_center_x500.pth").to(self.device)        # origin = torch.zeros(1, 3, 256, 256, device=self.device)        # for c in range(3):        #     j = random.randint(0,102)        #     print("get hyperplane:", j)        #     boundary_z_path = './test2_center/subspace_500_origin/human_coef_x'+str(j)+'.npy'        #     boundary_h_path = './test2_center/subspace_500_origin/human_coef_h'+str(j)+'.npy'        #     coef_x1 = np.load(boundary_z_path)        #     coef_h1 = np.load(boundary_h_path)          #     normals_z.append(torch.from_numpy(coef_x1).to(self.device).squeeze(0))        #     normals_h.append(torch.from_numpy(coef_h1).to(self.device).squeeze(0))        #     # print("check size:", normals_z[0].size(), normals_h[0].size())        # normals_z = torch.stack(normals_z, dim=0)        # normals_h = torch.stack(normals_h, dim=0)            # # print("check normals:", normals_z.size(), normals_h.size(), x_s.view(1,-1).size(), h_s.view(-1,1).size())              # subspace_point_z = point_to_subspace(x_s.view(-1), normals_z)        # subspace_point_z = point_to_subspace(local_center_x.view(-1), normals_z)        # subspace_point_h = point_to_subspace(h_s.view(-1), normals_h)        # # print("check subspace_point_z:", subspace_point_z.size(), subspace_point_h.size())        # edit_z = subspace_point_z.view(-1, 3, 256, 256)        # edit_h = subspace_point_h.view(-1, 512, 8, 8)        # edit_z = local_center_x        # edit_z = origin        # with torch.no_grad():        #     # for k in range(len(edit_z_seq)):        #     time_in_start = time.time()        #     with tqdm(total=len(seq_inv), desc="Generative process {}".format(it)) as progress_bar:        #         # edit_h = torch.from_numpy(edit_h_seq[k]).to(self.device).view(-1, 512, 8, 8)        #         # edit_z = torch.from_numpy(edit_z_seq[k]).to(self.device).view(-1, 3, 256, 256)        #         for i, j in zip(reversed(seq_inv), reversed(seq_inv_next)):        #             t = (torch.ones(n) * i).to(self.device)        #             t_next = (torch.ones(n) * j).to(self.device)        #             edit_z, edit_h = denoising_step(edit_z, t=t, t_next=t_next, models=model,        #                                logvars=self.logvar,        #                                sampling_type=self.args.sample_type,        #                                b=self.betas,        #                                eta = self.args.eta,        #                                learn_sigma=learn_sigma,        #                                ratio=self.args.model_ratio,        #                                hybrid=self.args.hybrid_noise,        #                                hybrid_config=HYBRID_CONFIG,        #                                # edit_h=edit_h,        #                                )        #     # x0 = x.clone()        #     # save_edit = "unseen_traj"+str(c)+".png"        #     save_edit = "unseen_traj.png"        #     tvu.save_image((edit_z + 1) * 0.5, os.path.join("projection_result",save_edit))        #     time_in_end = time.time()        #     print(f"Editing for 1 image takes {time_in_end - time_in_start:.4f}s")        return None